{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpKXtISNidUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9c1ef6d-aa76-4893-d4da-20aebf891efa"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "def softmax(a):\n",
        "  if a.ndim == 2:\n",
        "    a = a.T\n",
        "    c = np.max(a, axis = 0)\n",
        "    exp_a = np.exp(a - c) # 오버 플로우를 방지하기 위해 c를 빼준다.\n",
        "    exp_a = exp_a / np.sum(exp_a, axis=0)\n",
        "    return exp_a.T\n",
        "\n",
        "  c = np.max(a)\n",
        "  exp_a = np.exp(a - c)\n",
        "  return exp_a / np.sum(exp_a)\n",
        "\n",
        "def cross_entropy_error(x, t):\n",
        "  return -np.sum(t * np.log(x)) / x.shape[0]\n",
        "\n",
        "# 수치 미분 함수\n",
        "def numerical_gradient(func, param):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(param)\n",
        "  expand = False\n",
        "\n",
        "  if param.ndim == 1:\n",
        "    param = np.expand_dims(param, axis=0)\n",
        "    grad = np.expand_dims(grad, axis=0)\n",
        "    expand = True\n",
        "\n",
        "  for x in range(param.shape[0]):\n",
        "    for y in range(param.shape[1]):\n",
        "      tmp_val = param[x, y]\n",
        "      param[x, y] = tmp_val + h\n",
        "      fxh1 = func(param)\n",
        "\n",
        "      param[x, y] = tmp_val - h\n",
        "      fxh2 = func(param)\n",
        "\n",
        "      grad[x, y] = (fxh1 - fxh2) / 2*h\n",
        "      param[x, y] = tmp_val\n",
        "\n",
        "  if expand:\n",
        "    param = param.reshape(-1)\n",
        "    grad = grad.reshape(-1)\n",
        "  \n",
        "  return grad\n",
        "\n",
        "# affine transformation\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    self.dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis=0) \n",
        "     \n",
        "    return self.dx\n",
        "\n",
        "# 렐루 층\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dout\n",
        "\n",
        "# 소프트맥스 함수와, 손실함수가 합쳐진 층\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    return (self.y - self.t) / batch_size\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01, lr=0.01, batch_size=100, epochs=500):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    self.lr = lr\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis = 1)\n",
        "    accuracy = np.sum(y == np.argmax(t, axis=1))\n",
        "    return accuracy / float(y.shape[0])\n",
        "\n",
        "  # 수치미분\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W : self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    return grads\n",
        "\n",
        "  # SGD를 통해 가중치 수정값 계산하기\n",
        "  def gradient(self, x, t):\n",
        "    self.loss(x, t) # 순전파 구현하기\n",
        "\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "    \n",
        "    return grads\n",
        "\n",
        "  # 수치미분과, 해석적미분 값을 비교해서 SGD를 검증한다.\n",
        "  def verify_grad(self, x, t):\n",
        "    print(\"[ 경사하강법을 검증합니다. ]\")\n",
        "    x_val = x[:3]\n",
        "    t_val = t[:3]\n",
        "\n",
        "    grad = self.gradient(x_val, t_val)\n",
        "    grad_num = self.numerical_gradient(x_val, t_val)\n",
        "\n",
        "    for key in grad.keys():\n",
        "      print(str(key) + \" : %.10lf\" % np.mean(grad[key] - grad_num[key]))\n",
        "  \n",
        "  # 배치 생성 함수\n",
        "  def generate_batch(self, x, t):\n",
        "    random_mask = np.random.permutation(range(x.shape[0]))\n",
        "    x = x[random_mask]\n",
        "    t = t[random_mask]\n",
        "\n",
        "    batch_iters = int(x.shape[0] / self.batch_size)\n",
        "\n",
        "    for i in range(batch_iters):\n",
        "      start = i * self.batch_size\n",
        "      end = (i+1) * self.batch_size\n",
        "      yield x[start:end], t[start:end]\n",
        "  \n",
        "  # 학습 함수\n",
        "  def fit(self, x, t):\n",
        "    print()\n",
        "    print(\"============= 학습 시작 =============\")\n",
        "    history = {}\n",
        "    history['loss'] = []\n",
        "    history['acc'] = []\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      train_batch_loss = []\n",
        "      train_batch_acc = []\n",
        "      for x_batch, t_batch in self.generate_batch(x, t):\n",
        "        grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "          self.params[key] -= self.lr * grad[key]\n",
        "        \n",
        "        train_batch_loss.append(self.loss(x_batch, t_batch))\n",
        "        train_batch_acc.append(self.accuracy(x_batch, t_batch))\n",
        "      loss_mean = np.mean(train_batch_loss)\n",
        "      acc_mean = np.mean(train_batch_acc)\n",
        "\n",
        "      history['loss'].append(loss_mean)\n",
        "      history['acc'].append(acc_mean)\n",
        "      print(\"epochs: %d; loss: %f; accuracy: %f\" % (i, loss_mean, acc_mean))\n",
        "    return history\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "t_train = to_categorical(t_train)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) # 모델 구축\n",
        "network.verify_grad(x_train, t_train) # SGD 코드를 검사한다.\n",
        "history = network.fit(x_train, t_train) # 학습 시행\n",
        "\n",
        "# 결과값 그리기\n",
        "plt.plot(range(network.epochs), history['loss'], label=\"train loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(network.epochs), history['acc'], label=\"train acc\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 경사하강법을 검증합니다. ]\n",
            "W1 : 0.0001695026\n",
            "b1 : 0.0011716272\n",
            "W2 : -0.0000000000\n",
            "b2 : -0.0000000000\n",
            "\n",
            "============= 학습 시작 =============\n",
            "epochs: 0; loss: 2.228223; accuracy: 0.431350\n",
            "epochs: 1; loss: 1.342075; accuracy: 0.702650\n",
            "epochs: 2; loss: 0.713305; accuracy: 0.821283\n",
            "epochs: 3; loss: 0.531513; accuracy: 0.861500\n",
            "epochs: 4; loss: 0.451468; accuracy: 0.879250\n",
            "epochs: 5; loss: 0.407398; accuracy: 0.889083\n",
            "epochs: 6; loss: 0.379289; accuracy: 0.894800\n",
            "epochs: 7; loss: 0.359285; accuracy: 0.899600\n",
            "epochs: 8; loss: 0.344327; accuracy: 0.903133\n",
            "epochs: 9; loss: 0.332020; accuracy: 0.905833\n",
            "epochs: 10; loss: 0.321884; accuracy: 0.908983\n",
            "epochs: 11; loss: 0.313065; accuracy: 0.911367\n",
            "epochs: 12; loss: 0.305127; accuracy: 0.913667\n",
            "epochs: 13; loss: 0.298049; accuracy: 0.915600\n",
            "epochs: 14; loss: 0.291386; accuracy: 0.917567\n",
            "epochs: 15; loss: 0.285155; accuracy: 0.919667\n",
            "epochs: 16; loss: 0.279398; accuracy: 0.921533\n",
            "epochs: 17; loss: 0.273919; accuracy: 0.922483\n",
            "epochs: 18; loss: 0.268649; accuracy: 0.924467\n",
            "epochs: 19; loss: 0.263639; accuracy: 0.925550\n",
            "epochs: 20; loss: 0.258691; accuracy: 0.926767\n",
            "epochs: 21; loss: 0.253802; accuracy: 0.928300\n",
            "epochs: 22; loss: 0.249331; accuracy: 0.929917\n",
            "epochs: 23; loss: 0.244830; accuracy: 0.931333\n",
            "epochs: 24; loss: 0.240488; accuracy: 0.932467\n",
            "epochs: 25; loss: 0.236467; accuracy: 0.934283\n",
            "epochs: 26; loss: 0.232351; accuracy: 0.935200\n",
            "epochs: 27; loss: 0.228412; accuracy: 0.936333\n",
            "epochs: 28; loss: 0.224482; accuracy: 0.937433\n",
            "epochs: 29; loss: 0.220774; accuracy: 0.939317\n",
            "epochs: 30; loss: 0.217269; accuracy: 0.939900\n",
            "epochs: 31; loss: 0.213617; accuracy: 0.940967\n",
            "epochs: 32; loss: 0.210408; accuracy: 0.942167\n",
            "epochs: 33; loss: 0.207240; accuracy: 0.942700\n",
            "epochs: 34; loss: 0.204057; accuracy: 0.943733\n",
            "epochs: 35; loss: 0.201050; accuracy: 0.944817\n",
            "epochs: 36; loss: 0.198011; accuracy: 0.945450\n",
            "epochs: 37; loss: 0.195107; accuracy: 0.946333\n",
            "epochs: 38; loss: 0.192216; accuracy: 0.946867\n",
            "epochs: 39; loss: 0.189555; accuracy: 0.947650\n",
            "epochs: 40; loss: 0.186884; accuracy: 0.948617\n",
            "epochs: 41; loss: 0.184265; accuracy: 0.949450\n",
            "epochs: 42; loss: 0.181818; accuracy: 0.949917\n",
            "epochs: 43; loss: 0.179192; accuracy: 0.950617\n",
            "epochs: 44; loss: 0.176823; accuracy: 0.951400\n",
            "epochs: 45; loss: 0.174572; accuracy: 0.952050\n",
            "epochs: 46; loss: 0.172347; accuracy: 0.952650\n",
            "epochs: 47; loss: 0.170223; accuracy: 0.953317\n",
            "epochs: 48; loss: 0.168049; accuracy: 0.953533\n",
            "epochs: 49; loss: 0.165871; accuracy: 0.954383\n",
            "epochs: 50; loss: 0.163956; accuracy: 0.955100\n",
            "epochs: 51; loss: 0.161823; accuracy: 0.955617\n",
            "epochs: 52; loss: 0.159922; accuracy: 0.955750\n",
            "epochs: 53; loss: 0.157994; accuracy: 0.956467\n",
            "epochs: 54; loss: 0.156197; accuracy: 0.956983\n",
            "epochs: 55; loss: 0.154288; accuracy: 0.957233\n",
            "epochs: 56; loss: 0.152598; accuracy: 0.957917\n",
            "epochs: 57; loss: 0.150776; accuracy: 0.958150\n",
            "epochs: 58; loss: 0.149156; accuracy: 0.958667\n",
            "epochs: 59; loss: 0.147484; accuracy: 0.959233\n",
            "epochs: 60; loss: 0.145783; accuracy: 0.959767\n",
            "epochs: 61; loss: 0.144269; accuracy: 0.959950\n",
            "epochs: 62; loss: 0.142752; accuracy: 0.960350\n",
            "epochs: 63; loss: 0.141162; accuracy: 0.961067\n",
            "epochs: 64; loss: 0.139602; accuracy: 0.961117\n",
            "epochs: 65; loss: 0.138106; accuracy: 0.961683\n",
            "epochs: 66; loss: 0.136751; accuracy: 0.962267\n",
            "epochs: 67; loss: 0.135180; accuracy: 0.962317\n",
            "epochs: 68; loss: 0.133901; accuracy: 0.962917\n",
            "epochs: 69; loss: 0.132418; accuracy: 0.963317\n",
            "epochs: 70; loss: 0.131095; accuracy: 0.963717\n",
            "epochs: 71; loss: 0.129864; accuracy: 0.964283\n",
            "epochs: 72; loss: 0.128588; accuracy: 0.964617\n",
            "epochs: 73; loss: 0.127322; accuracy: 0.964950\n",
            "epochs: 74; loss: 0.126149; accuracy: 0.965383\n",
            "epochs: 75; loss: 0.124850; accuracy: 0.965667\n",
            "epochs: 76; loss: 0.123841; accuracy: 0.966150\n",
            "epochs: 77; loss: 0.122595; accuracy: 0.966267\n",
            "epochs: 78; loss: 0.121472; accuracy: 0.966767\n",
            "epochs: 79; loss: 0.120320; accuracy: 0.967100\n",
            "epochs: 80; loss: 0.119224; accuracy: 0.967667\n",
            "epochs: 81; loss: 0.118211; accuracy: 0.967583\n",
            "epochs: 82; loss: 0.117123; accuracy: 0.967933\n",
            "epochs: 83; loss: 0.116112; accuracy: 0.968500\n",
            "epochs: 84; loss: 0.115027; accuracy: 0.968733\n",
            "epochs: 85; loss: 0.114089; accuracy: 0.968833\n",
            "epochs: 86; loss: 0.113163; accuracy: 0.969167\n",
            "epochs: 87; loss: 0.112226; accuracy: 0.969600\n",
            "epochs: 88; loss: 0.111216; accuracy: 0.969867\n",
            "epochs: 89; loss: 0.110210; accuracy: 0.969900\n",
            "epochs: 90; loss: 0.109371; accuracy: 0.970033\n",
            "epochs: 91; loss: 0.108442; accuracy: 0.970267\n",
            "epochs: 92; loss: 0.107578; accuracy: 0.970233\n",
            "epochs: 93; loss: 0.106730; accuracy: 0.970950\n",
            "epochs: 94; loss: 0.105851; accuracy: 0.971200\n",
            "epochs: 95; loss: 0.105014; accuracy: 0.971400\n",
            "epochs: 96; loss: 0.104240; accuracy: 0.971633\n",
            "epochs: 97; loss: 0.103360; accuracy: 0.971883\n",
            "epochs: 98; loss: 0.102571; accuracy: 0.972100\n",
            "epochs: 99; loss: 0.101768; accuracy: 0.972267\n",
            "epochs: 100; loss: 0.100999; accuracy: 0.972767\n",
            "epochs: 101; loss: 0.100267; accuracy: 0.973083\n",
            "epochs: 102; loss: 0.099483; accuracy: 0.973033\n",
            "epochs: 103; loss: 0.098749; accuracy: 0.973350\n",
            "epochs: 104; loss: 0.097985; accuracy: 0.973633\n",
            "epochs: 105; loss: 0.097216; accuracy: 0.973650\n",
            "epochs: 106; loss: 0.096585; accuracy: 0.973750\n",
            "epochs: 107; loss: 0.095837; accuracy: 0.973967\n",
            "epochs: 108; loss: 0.095161; accuracy: 0.974217\n",
            "epochs: 109; loss: 0.094506; accuracy: 0.974300\n",
            "epochs: 110; loss: 0.093794; accuracy: 0.974567\n",
            "epochs: 111; loss: 0.093120; accuracy: 0.974850\n",
            "epochs: 112; loss: 0.092467; accuracy: 0.974750\n",
            "epochs: 113; loss: 0.091853; accuracy: 0.974983\n",
            "epochs: 114; loss: 0.091145; accuracy: 0.975450\n",
            "epochs: 115; loss: 0.090524; accuracy: 0.975500\n",
            "epochs: 116; loss: 0.089950; accuracy: 0.975483\n",
            "epochs: 117; loss: 0.089305; accuracy: 0.975833\n",
            "epochs: 118; loss: 0.088769; accuracy: 0.976000\n",
            "epochs: 119; loss: 0.088118; accuracy: 0.976133\n",
            "epochs: 120; loss: 0.087497; accuracy: 0.976167\n",
            "epochs: 121; loss: 0.086965; accuracy: 0.976333\n",
            "epochs: 122; loss: 0.086363; accuracy: 0.976850\n",
            "epochs: 123; loss: 0.085772; accuracy: 0.976900\n",
            "epochs: 124; loss: 0.085293; accuracy: 0.976783\n",
            "epochs: 125; loss: 0.084643; accuracy: 0.977217\n",
            "epochs: 126; loss: 0.084182; accuracy: 0.977133\n",
            "epochs: 127; loss: 0.083624; accuracy: 0.977417\n",
            "epochs: 128; loss: 0.083088; accuracy: 0.977550\n",
            "epochs: 129; loss: 0.082605; accuracy: 0.977600\n",
            "epochs: 130; loss: 0.082130; accuracy: 0.977750\n",
            "epochs: 131; loss: 0.081564; accuracy: 0.978083\n",
            "epochs: 132; loss: 0.081082; accuracy: 0.978083\n",
            "epochs: 133; loss: 0.080493; accuracy: 0.978283\n",
            "epochs: 134; loss: 0.080109; accuracy: 0.978333\n",
            "epochs: 135; loss: 0.079603; accuracy: 0.978500\n",
            "epochs: 136; loss: 0.079114; accuracy: 0.978383\n",
            "epochs: 137; loss: 0.078667; accuracy: 0.978683\n",
            "epochs: 138; loss: 0.078124; accuracy: 0.978800\n",
            "epochs: 139; loss: 0.077645; accuracy: 0.979000\n",
            "epochs: 140; loss: 0.077173; accuracy: 0.979017\n",
            "epochs: 141; loss: 0.076772; accuracy: 0.979617\n",
            "epochs: 142; loss: 0.076249; accuracy: 0.979283\n",
            "epochs: 143; loss: 0.075815; accuracy: 0.979300\n",
            "epochs: 144; loss: 0.075388; accuracy: 0.979517\n",
            "epochs: 145; loss: 0.075007; accuracy: 0.979800\n",
            "epochs: 146; loss: 0.074548; accuracy: 0.979917\n",
            "epochs: 147; loss: 0.074163; accuracy: 0.980033\n",
            "epochs: 148; loss: 0.073678; accuracy: 0.979950\n",
            "epochs: 149; loss: 0.073294; accuracy: 0.980233\n",
            "epochs: 150; loss: 0.072824; accuracy: 0.980617\n",
            "epochs: 151; loss: 0.072425; accuracy: 0.980400\n",
            "epochs: 152; loss: 0.071988; accuracy: 0.980583\n",
            "epochs: 153; loss: 0.071563; accuracy: 0.981000\n",
            "epochs: 154; loss: 0.071156; accuracy: 0.980800\n",
            "epochs: 155; loss: 0.070762; accuracy: 0.981000\n",
            "epochs: 156; loss: 0.070342; accuracy: 0.981300\n",
            "epochs: 157; loss: 0.070004; accuracy: 0.981233\n",
            "epochs: 158; loss: 0.069633; accuracy: 0.981250\n",
            "epochs: 159; loss: 0.069245; accuracy: 0.981383\n",
            "epochs: 160; loss: 0.068868; accuracy: 0.981600\n",
            "epochs: 161; loss: 0.068421; accuracy: 0.981483\n",
            "epochs: 162; loss: 0.068063; accuracy: 0.981683\n",
            "epochs: 163; loss: 0.067790; accuracy: 0.981883\n",
            "epochs: 164; loss: 0.067367; accuracy: 0.982000\n",
            "epochs: 165; loss: 0.066985; accuracy: 0.982067\n",
            "epochs: 166; loss: 0.066655; accuracy: 0.982217\n",
            "epochs: 167; loss: 0.066328; accuracy: 0.982217\n",
            "epochs: 168; loss: 0.066062; accuracy: 0.982367\n",
            "epochs: 169; loss: 0.065581; accuracy: 0.982450\n",
            "epochs: 170; loss: 0.065241; accuracy: 0.982650\n",
            "epochs: 171; loss: 0.064921; accuracy: 0.982533\n",
            "epochs: 172; loss: 0.064543; accuracy: 0.982800\n",
            "epochs: 173; loss: 0.064268; accuracy: 0.982467\n",
            "epochs: 174; loss: 0.063903; accuracy: 0.983050\n",
            "epochs: 175; loss: 0.063587; accuracy: 0.982950\n",
            "epochs: 176; loss: 0.063292; accuracy: 0.983050\n",
            "epochs: 177; loss: 0.062973; accuracy: 0.983150\n",
            "epochs: 178; loss: 0.062619; accuracy: 0.983300\n",
            "epochs: 179; loss: 0.062304; accuracy: 0.983233\n",
            "epochs: 180; loss: 0.061900; accuracy: 0.983483\n",
            "epochs: 181; loss: 0.061660; accuracy: 0.983567\n",
            "epochs: 182; loss: 0.061310; accuracy: 0.983483\n",
            "epochs: 183; loss: 0.061001; accuracy: 0.983883\n",
            "epochs: 184; loss: 0.060764; accuracy: 0.983733\n",
            "epochs: 185; loss: 0.060424; accuracy: 0.983900\n",
            "epochs: 186; loss: 0.060115; accuracy: 0.984117\n",
            "epochs: 187; loss: 0.059825; accuracy: 0.983817\n",
            "epochs: 188; loss: 0.059489; accuracy: 0.984233\n",
            "epochs: 189; loss: 0.059169; accuracy: 0.984300\n",
            "epochs: 190; loss: 0.058937; accuracy: 0.984700\n",
            "epochs: 191; loss: 0.058643; accuracy: 0.984617\n",
            "epochs: 192; loss: 0.058301; accuracy: 0.984517\n",
            "epochs: 193; loss: 0.058080; accuracy: 0.984583\n",
            "epochs: 194; loss: 0.057745; accuracy: 0.984717\n",
            "epochs: 195; loss: 0.057518; accuracy: 0.984867\n",
            "epochs: 196; loss: 0.057251; accuracy: 0.985000\n",
            "epochs: 197; loss: 0.056980; accuracy: 0.985100\n",
            "epochs: 198; loss: 0.056613; accuracy: 0.984817\n",
            "epochs: 199; loss: 0.056458; accuracy: 0.985150\n",
            "epochs: 200; loss: 0.056132; accuracy: 0.985283\n",
            "epochs: 201; loss: 0.055868; accuracy: 0.985217\n",
            "epochs: 202; loss: 0.055600; accuracy: 0.985417\n",
            "epochs: 203; loss: 0.055363; accuracy: 0.985567\n",
            "epochs: 204; loss: 0.055107; accuracy: 0.985633\n",
            "epochs: 205; loss: 0.054834; accuracy: 0.985867\n",
            "epochs: 206; loss: 0.054581; accuracy: 0.985700\n",
            "epochs: 207; loss: 0.054288; accuracy: 0.985750\n",
            "epochs: 208; loss: 0.054056; accuracy: 0.985850\n",
            "epochs: 209; loss: 0.053839; accuracy: 0.985983\n",
            "epochs: 210; loss: 0.053558; accuracy: 0.986100\n",
            "epochs: 211; loss: 0.053302; accuracy: 0.986100\n",
            "epochs: 212; loss: 0.053047; accuracy: 0.986317\n",
            "epochs: 213; loss: 0.052802; accuracy: 0.986233\n",
            "epochs: 214; loss: 0.052585; accuracy: 0.986267\n",
            "epochs: 215; loss: 0.052381; accuracy: 0.986433\n",
            "epochs: 216; loss: 0.052131; accuracy: 0.986500\n",
            "epochs: 217; loss: 0.051786; accuracy: 0.986600\n",
            "epochs: 218; loss: 0.051620; accuracy: 0.986717\n",
            "epochs: 219; loss: 0.051414; accuracy: 0.986833\n",
            "epochs: 220; loss: 0.051087; accuracy: 0.986917\n",
            "epochs: 221; loss: 0.050878; accuracy: 0.986917\n",
            "epochs: 222; loss: 0.050703; accuracy: 0.986900\n",
            "epochs: 223; loss: 0.050512; accuracy: 0.987000\n",
            "epochs: 224; loss: 0.050271; accuracy: 0.987200\n",
            "epochs: 225; loss: 0.050023; accuracy: 0.987300\n",
            "epochs: 226; loss: 0.049839; accuracy: 0.987450\n",
            "epochs: 227; loss: 0.049553; accuracy: 0.987550\n",
            "epochs: 228; loss: 0.049386; accuracy: 0.987433\n",
            "epochs: 229; loss: 0.049161; accuracy: 0.987450\n",
            "epochs: 230; loss: 0.048940; accuracy: 0.987483\n",
            "epochs: 231; loss: 0.048718; accuracy: 0.987650\n",
            "epochs: 232; loss: 0.048467; accuracy: 0.987700\n",
            "epochs: 233; loss: 0.048287; accuracy: 0.987967\n",
            "epochs: 234; loss: 0.048069; accuracy: 0.987767\n",
            "epochs: 235; loss: 0.047830; accuracy: 0.988050\n",
            "epochs: 236; loss: 0.047598; accuracy: 0.988200\n",
            "epochs: 237; loss: 0.047441; accuracy: 0.988083\n",
            "epochs: 238; loss: 0.047297; accuracy: 0.988250\n",
            "epochs: 239; loss: 0.047052; accuracy: 0.988233\n",
            "epochs: 240; loss: 0.046766; accuracy: 0.988167\n",
            "epochs: 241; loss: 0.046640; accuracy: 0.988383\n",
            "epochs: 242; loss: 0.046423; accuracy: 0.988483\n",
            "epochs: 243; loss: 0.046204; accuracy: 0.988550\n",
            "epochs: 244; loss: 0.046002; accuracy: 0.988650\n",
            "epochs: 245; loss: 0.045852; accuracy: 0.988583\n",
            "epochs: 246; loss: 0.045696; accuracy: 0.988600\n",
            "epochs: 247; loss: 0.045369; accuracy: 0.988883\n",
            "epochs: 248; loss: 0.045321; accuracy: 0.988817\n",
            "epochs: 249; loss: 0.045051; accuracy: 0.989033\n",
            "epochs: 250; loss: 0.044851; accuracy: 0.988900\n",
            "epochs: 251; loss: 0.044706; accuracy: 0.988917\n",
            "epochs: 252; loss: 0.044501; accuracy: 0.989050\n",
            "epochs: 253; loss: 0.044331; accuracy: 0.989183\n",
            "epochs: 254; loss: 0.044130; accuracy: 0.989433\n",
            "epochs: 255; loss: 0.043948; accuracy: 0.989250\n",
            "epochs: 256; loss: 0.043746; accuracy: 0.989333\n",
            "epochs: 257; loss: 0.043607; accuracy: 0.989267\n",
            "epochs: 258; loss: 0.043387; accuracy: 0.989567\n",
            "epochs: 259; loss: 0.043187; accuracy: 0.989650\n",
            "epochs: 260; loss: 0.043014; accuracy: 0.989550\n",
            "epochs: 261; loss: 0.042866; accuracy: 0.989633\n",
            "epochs: 262; loss: 0.042719; accuracy: 0.989733\n",
            "epochs: 263; loss: 0.042512; accuracy: 0.989683\n",
            "epochs: 264; loss: 0.042374; accuracy: 0.989650\n",
            "epochs: 265; loss: 0.042126; accuracy: 0.989767\n",
            "epochs: 266; loss: 0.041970; accuracy: 0.989933\n",
            "epochs: 267; loss: 0.041810; accuracy: 0.990250\n",
            "epochs: 268; loss: 0.041633; accuracy: 0.990017\n",
            "epochs: 269; loss: 0.041492; accuracy: 0.990100\n",
            "epochs: 270; loss: 0.041274; accuracy: 0.990050\n",
            "epochs: 271; loss: 0.041164; accuracy: 0.990350\n",
            "epochs: 272; loss: 0.041019; accuracy: 0.990217\n",
            "epochs: 273; loss: 0.040761; accuracy: 0.990433\n",
            "epochs: 274; loss: 0.040685; accuracy: 0.990317\n",
            "epochs: 275; loss: 0.040457; accuracy: 0.990283\n",
            "epochs: 276; loss: 0.040319; accuracy: 0.990567\n",
            "epochs: 277; loss: 0.040131; accuracy: 0.990483\n",
            "epochs: 278; loss: 0.039993; accuracy: 0.990467\n",
            "epochs: 279; loss: 0.039860; accuracy: 0.990733\n",
            "epochs: 280; loss: 0.039750; accuracy: 0.990700\n",
            "epochs: 281; loss: 0.039481; accuracy: 0.990717\n",
            "epochs: 282; loss: 0.039365; accuracy: 0.990750\n",
            "epochs: 283; loss: 0.039195; accuracy: 0.990850\n",
            "epochs: 284; loss: 0.039037; accuracy: 0.991017\n",
            "epochs: 285; loss: 0.038876; accuracy: 0.990933\n",
            "epochs: 286; loss: 0.038740; accuracy: 0.991000\n",
            "epochs: 287; loss: 0.038592; accuracy: 0.990800\n",
            "epochs: 288; loss: 0.038473; accuracy: 0.991100\n",
            "epochs: 289; loss: 0.038322; accuracy: 0.991217\n",
            "epochs: 290; loss: 0.038095; accuracy: 0.991333\n",
            "epochs: 291; loss: 0.037997; accuracy: 0.991333\n",
            "epochs: 292; loss: 0.037854; accuracy: 0.991400\n",
            "epochs: 293; loss: 0.037722; accuracy: 0.991283\n",
            "epochs: 294; loss: 0.037552; accuracy: 0.991500\n",
            "epochs: 295; loss: 0.037438; accuracy: 0.991350\n",
            "epochs: 296; loss: 0.037271; accuracy: 0.991533\n",
            "epochs: 297; loss: 0.037075; accuracy: 0.991567\n",
            "epochs: 298; loss: 0.036997; accuracy: 0.991550\n",
            "epochs: 299; loss: 0.036806; accuracy: 0.991783\n",
            "epochs: 300; loss: 0.036706; accuracy: 0.991717\n",
            "epochs: 301; loss: 0.036542; accuracy: 0.991817\n",
            "epochs: 302; loss: 0.036429; accuracy: 0.991700\n",
            "epochs: 303; loss: 0.036251; accuracy: 0.991783\n",
            "epochs: 304; loss: 0.036101; accuracy: 0.991867\n",
            "epochs: 305; loss: 0.035937; accuracy: 0.991967\n",
            "epochs: 306; loss: 0.035833; accuracy: 0.992017\n",
            "epochs: 307; loss: 0.035739; accuracy: 0.992050\n",
            "epochs: 308; loss: 0.035581; accuracy: 0.992183\n",
            "epochs: 309; loss: 0.035464; accuracy: 0.992033\n",
            "epochs: 310; loss: 0.035327; accuracy: 0.991950\n",
            "epochs: 311; loss: 0.035201; accuracy: 0.992100\n",
            "epochs: 312; loss: 0.035054; accuracy: 0.992350\n",
            "epochs: 313; loss: 0.034888; accuracy: 0.992417\n",
            "epochs: 314; loss: 0.034794; accuracy: 0.992383\n",
            "epochs: 315; loss: 0.034623; accuracy: 0.992350\n",
            "epochs: 316; loss: 0.034514; accuracy: 0.992400\n",
            "epochs: 317; loss: 0.034401; accuracy: 0.992517\n",
            "epochs: 318; loss: 0.034262; accuracy: 0.992483\n",
            "epochs: 319; loss: 0.034143; accuracy: 0.992517\n",
            "epochs: 320; loss: 0.034004; accuracy: 0.992583\n",
            "epochs: 321; loss: 0.033914; accuracy: 0.992650\n",
            "epochs: 322; loss: 0.033771; accuracy: 0.992500\n",
            "epochs: 323; loss: 0.033573; accuracy: 0.992617\n",
            "epochs: 324; loss: 0.033489; accuracy: 0.992900\n",
            "epochs: 325; loss: 0.033352; accuracy: 0.992850\n",
            "epochs: 326; loss: 0.033277; accuracy: 0.992667\n",
            "epochs: 327; loss: 0.033131; accuracy: 0.992717\n",
            "epochs: 328; loss: 0.033012; accuracy: 0.993000\n",
            "epochs: 329; loss: 0.032890; accuracy: 0.993017\n",
            "epochs: 330; loss: 0.032782; accuracy: 0.992850\n",
            "epochs: 331; loss: 0.032623; accuracy: 0.993017\n",
            "epochs: 332; loss: 0.032486; accuracy: 0.992983\n",
            "epochs: 333; loss: 0.032417; accuracy: 0.993017\n",
            "epochs: 334; loss: 0.032297; accuracy: 0.993267\n",
            "epochs: 335; loss: 0.032156; accuracy: 0.993083\n",
            "epochs: 336; loss: 0.032028; accuracy: 0.993133\n",
            "epochs: 337; loss: 0.031924; accuracy: 0.993233\n",
            "epochs: 338; loss: 0.031808; accuracy: 0.993400\n",
            "epochs: 339; loss: 0.031720; accuracy: 0.993233\n",
            "epochs: 340; loss: 0.031570; accuracy: 0.993350\n",
            "epochs: 341; loss: 0.031477; accuracy: 0.993417\n",
            "epochs: 342; loss: 0.031338; accuracy: 0.993383\n",
            "epochs: 343; loss: 0.031205; accuracy: 0.993417\n",
            "epochs: 344; loss: 0.031107; accuracy: 0.993500\n",
            "epochs: 345; loss: 0.030988; accuracy: 0.993417\n",
            "epochs: 346; loss: 0.030895; accuracy: 0.993467\n",
            "epochs: 347; loss: 0.030799; accuracy: 0.993600\n",
            "epochs: 348; loss: 0.030651; accuracy: 0.993583\n",
            "epochs: 349; loss: 0.030563; accuracy: 0.993667\n",
            "epochs: 350; loss: 0.030425; accuracy: 0.993600\n",
            "epochs: 351; loss: 0.030331; accuracy: 0.993650\n",
            "epochs: 352; loss: 0.030207; accuracy: 0.993817\n",
            "epochs: 353; loss: 0.030088; accuracy: 0.993867\n",
            "epochs: 354; loss: 0.029996; accuracy: 0.993983\n",
            "epochs: 355; loss: 0.029915; accuracy: 0.993950\n",
            "epochs: 356; loss: 0.029801; accuracy: 0.994033\n",
            "epochs: 357; loss: 0.029683; accuracy: 0.994117\n",
            "epochs: 358; loss: 0.029546; accuracy: 0.994000\n",
            "epochs: 359; loss: 0.029490; accuracy: 0.994117\n",
            "epochs: 360; loss: 0.029403; accuracy: 0.994250\n",
            "epochs: 361; loss: 0.029248; accuracy: 0.994067\n",
            "epochs: 362; loss: 0.029201; accuracy: 0.994133\n",
            "epochs: 363; loss: 0.029075; accuracy: 0.994250\n",
            "epochs: 364; loss: 0.028962; accuracy: 0.994067\n",
            "epochs: 365; loss: 0.028827; accuracy: 0.994167\n",
            "epochs: 366; loss: 0.028726; accuracy: 0.994183\n",
            "epochs: 367; loss: 0.028613; accuracy: 0.994233\n",
            "epochs: 368; loss: 0.028574; accuracy: 0.994433\n",
            "epochs: 369; loss: 0.028441; accuracy: 0.994517\n",
            "epochs: 370; loss: 0.028362; accuracy: 0.994333\n",
            "epochs: 371; loss: 0.028236; accuracy: 0.994333\n",
            "epochs: 372; loss: 0.028148; accuracy: 0.994433\n",
            "epochs: 373; loss: 0.028038; accuracy: 0.994400\n",
            "epochs: 374; loss: 0.027908; accuracy: 0.994483\n",
            "epochs: 375; loss: 0.027851; accuracy: 0.994600\n",
            "epochs: 376; loss: 0.027746; accuracy: 0.994517\n",
            "epochs: 377; loss: 0.027677; accuracy: 0.994600\n",
            "epochs: 378; loss: 0.027585; accuracy: 0.994583\n",
            "epochs: 379; loss: 0.027485; accuracy: 0.994600\n",
            "epochs: 380; loss: 0.027377; accuracy: 0.994483\n",
            "epochs: 381; loss: 0.027256; accuracy: 0.994833\n",
            "epochs: 382; loss: 0.027198; accuracy: 0.994700\n",
            "epochs: 383; loss: 0.027108; accuracy: 0.994733\n",
            "epochs: 384; loss: 0.027020; accuracy: 0.994683\n",
            "epochs: 385; loss: 0.026933; accuracy: 0.994650\n",
            "epochs: 386; loss: 0.026800; accuracy: 0.994817\n",
            "epochs: 387; loss: 0.026698; accuracy: 0.994933\n",
            "epochs: 388; loss: 0.026640; accuracy: 0.994783\n",
            "epochs: 389; loss: 0.026551; accuracy: 0.994817\n",
            "epochs: 390; loss: 0.026480; accuracy: 0.994783\n",
            "epochs: 391; loss: 0.026378; accuracy: 0.994883\n",
            "epochs: 392; loss: 0.026291; accuracy: 0.994933\n",
            "epochs: 393; loss: 0.026189; accuracy: 0.995017\n",
            "epochs: 394; loss: 0.026060; accuracy: 0.995283\n",
            "epochs: 395; loss: 0.026023; accuracy: 0.994983\n",
            "epochs: 396; loss: 0.025922; accuracy: 0.995217\n",
            "epochs: 397; loss: 0.025836; accuracy: 0.995250\n",
            "epochs: 398; loss: 0.025718; accuracy: 0.995167\n",
            "epochs: 399; loss: 0.025645; accuracy: 0.995250\n",
            "epochs: 400; loss: 0.025584; accuracy: 0.995167\n",
            "epochs: 401; loss: 0.025479; accuracy: 0.995150\n",
            "epochs: 402; loss: 0.025392; accuracy: 0.995300\n",
            "epochs: 403; loss: 0.025319; accuracy: 0.995350\n",
            "epochs: 404; loss: 0.025257; accuracy: 0.995300\n",
            "epochs: 405; loss: 0.025122; accuracy: 0.995367\n",
            "epochs: 406; loss: 0.025049; accuracy: 0.995467\n",
            "epochs: 407; loss: 0.024976; accuracy: 0.995383\n",
            "epochs: 408; loss: 0.024889; accuracy: 0.995483\n",
            "epochs: 409; loss: 0.024816; accuracy: 0.995517\n",
            "epochs: 410; loss: 0.024723; accuracy: 0.995417\n",
            "epochs: 411; loss: 0.024673; accuracy: 0.995533\n",
            "epochs: 412; loss: 0.024519; accuracy: 0.995550\n",
            "epochs: 413; loss: 0.024459; accuracy: 0.995650\n",
            "epochs: 414; loss: 0.024366; accuracy: 0.995650\n",
            "epochs: 415; loss: 0.024299; accuracy: 0.995667\n",
            "epochs: 416; loss: 0.024231; accuracy: 0.995633\n",
            "epochs: 417; loss: 0.024144; accuracy: 0.995683\n",
            "epochs: 418; loss: 0.024077; accuracy: 0.995783\n",
            "epochs: 419; loss: 0.023998; accuracy: 0.995683\n",
            "epochs: 420; loss: 0.023915; accuracy: 0.995733\n",
            "epochs: 421; loss: 0.023850; accuracy: 0.995750\n",
            "epochs: 422; loss: 0.023783; accuracy: 0.995900\n",
            "epochs: 423; loss: 0.023690; accuracy: 0.995900\n",
            "epochs: 424; loss: 0.023600; accuracy: 0.995883\n",
            "epochs: 425; loss: 0.023534; accuracy: 0.995800\n",
            "epochs: 426; loss: 0.023444; accuracy: 0.995867\n",
            "epochs: 427; loss: 0.023374; accuracy: 0.995950\n",
            "epochs: 428; loss: 0.023291; accuracy: 0.995933\n",
            "epochs: 429; loss: 0.023245; accuracy: 0.995967\n",
            "epochs: 430; loss: 0.023160; accuracy: 0.996033\n",
            "epochs: 431; loss: 0.023066; accuracy: 0.995983\n",
            "epochs: 432; loss: 0.022995; accuracy: 0.996100\n",
            "epochs: 433; loss: 0.022908; accuracy: 0.996100\n",
            "epochs: 434; loss: 0.022866; accuracy: 0.995967\n",
            "epochs: 435; loss: 0.022800; accuracy: 0.996000\n",
            "epochs: 436; loss: 0.022689; accuracy: 0.996050\n",
            "epochs: 437; loss: 0.022626; accuracy: 0.996033\n",
            "epochs: 438; loss: 0.022546; accuracy: 0.996267\n",
            "epochs: 439; loss: 0.022481; accuracy: 0.996183\n",
            "epochs: 440; loss: 0.022420; accuracy: 0.996317\n",
            "epochs: 441; loss: 0.022330; accuracy: 0.996150\n",
            "epochs: 442; loss: 0.022266; accuracy: 0.996217\n",
            "epochs: 443; loss: 0.022180; accuracy: 0.996317\n",
            "epochs: 444; loss: 0.022122; accuracy: 0.996350\n",
            "epochs: 445; loss: 0.022080; accuracy: 0.996333\n",
            "epochs: 446; loss: 0.021986; accuracy: 0.996350\n",
            "epochs: 447; loss: 0.021887; accuracy: 0.996417\n",
            "epochs: 448; loss: 0.021799; accuracy: 0.996367\n",
            "epochs: 449; loss: 0.021771; accuracy: 0.996350\n",
            "epochs: 450; loss: 0.021721; accuracy: 0.996483\n",
            "epochs: 451; loss: 0.021607; accuracy: 0.996383\n",
            "epochs: 452; loss: 0.021554; accuracy: 0.996600\n",
            "epochs: 453; loss: 0.021480; accuracy: 0.996467\n",
            "epochs: 454; loss: 0.021430; accuracy: 0.996600\n",
            "epochs: 455; loss: 0.021359; accuracy: 0.996567\n",
            "epochs: 456; loss: 0.021305; accuracy: 0.996550\n",
            "epochs: 457; loss: 0.021224; accuracy: 0.996567\n",
            "epochs: 458; loss: 0.021150; accuracy: 0.996583\n",
            "epochs: 459; loss: 0.021106; accuracy: 0.996550\n",
            "epochs: 460; loss: 0.021024; accuracy: 0.996617\n",
            "epochs: 461; loss: 0.020925; accuracy: 0.996600\n",
            "epochs: 462; loss: 0.020887; accuracy: 0.996733\n",
            "epochs: 463; loss: 0.020847; accuracy: 0.996783\n",
            "epochs: 464; loss: 0.020733; accuracy: 0.996833\n",
            "epochs: 465; loss: 0.020652; accuracy: 0.996833\n",
            "epochs: 466; loss: 0.020642; accuracy: 0.996700\n",
            "epochs: 467; loss: 0.020570; accuracy: 0.996817\n",
            "epochs: 468; loss: 0.020506; accuracy: 0.996783\n",
            "epochs: 469; loss: 0.020429; accuracy: 0.996783\n",
            "epochs: 470; loss: 0.020389; accuracy: 0.996717\n",
            "epochs: 471; loss: 0.020318; accuracy: 0.996933\n",
            "epochs: 472; loss: 0.020250; accuracy: 0.996717\n",
            "epochs: 473; loss: 0.020168; accuracy: 0.996900\n",
            "epochs: 474; loss: 0.020108; accuracy: 0.997000\n",
            "epochs: 475; loss: 0.020044; accuracy: 0.996900\n",
            "epochs: 476; loss: 0.019984; accuracy: 0.996933\n",
            "epochs: 477; loss: 0.019963; accuracy: 0.996933\n",
            "epochs: 478; loss: 0.019874; accuracy: 0.996883\n",
            "epochs: 479; loss: 0.019814; accuracy: 0.996900\n",
            "epochs: 480; loss: 0.019746; accuracy: 0.997100\n",
            "epochs: 481; loss: 0.019657; accuracy: 0.997017\n",
            "epochs: 482; loss: 0.019627; accuracy: 0.997100\n",
            "epochs: 483; loss: 0.019540; accuracy: 0.997117\n",
            "epochs: 484; loss: 0.019528; accuracy: 0.997017\n",
            "epochs: 485; loss: 0.019439; accuracy: 0.997150\n",
            "epochs: 486; loss: 0.019353; accuracy: 0.997117\n",
            "epochs: 487; loss: 0.019297; accuracy: 0.997183\n",
            "epochs: 488; loss: 0.019279; accuracy: 0.997133\n",
            "epochs: 489; loss: 0.019221; accuracy: 0.997267\n",
            "epochs: 490; loss: 0.019166; accuracy: 0.997150\n",
            "epochs: 491; loss: 0.019086; accuracy: 0.997183\n",
            "epochs: 492; loss: 0.019036; accuracy: 0.997250\n",
            "epochs: 493; loss: 0.018963; accuracy: 0.997233\n",
            "epochs: 494; loss: 0.018914; accuracy: 0.997233\n",
            "epochs: 495; loss: 0.018872; accuracy: 0.997383\n",
            "epochs: 496; loss: 0.018809; accuracy: 0.997233\n",
            "epochs: 497; loss: 0.018736; accuracy: 0.997283\n",
            "epochs: 498; loss: 0.018663; accuracy: 0.997417\n",
            "epochs: 499; loss: 0.018636; accuracy: 0.997400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbLElEQVR4nO3df3TcdZ3v8ef7OzNJmiY0NAmlUCCtgLSVUiCUYsWiZ+W0sBf04mXhiLAeF5a9uup1j1e4e0TZe7wHz+W4WkUQBAVR9Ci6FuxdFpBKd4Vi6BZaKNCWFpuCbZo2oWnza2Y+94/5TjKZmTRpMuk3n8nrcc6c+X4/38985/NN09f3k8/3lznnEBER/wVRN0BEREpDgS4iUiYU6CIiZUKBLiJSJhToIiJlIh7VFzc0NLimpqaovl5ExEsvvvjiPudcY7FlkQV6U1MTLS0tUX29iIiXzOyt4ZZpyEVEpEwo0EVEyoQCXUSkTEQ2hi4i5au/v5/W1lZ6enqiboq3qqqqmDNnDolEYtSfUaCLSMm1trZSW1tLU1MTZhZ1c7zjnKO9vZ3W1lbmzp076s9pyEVESq6np4f6+nqF+RiZGfX19Uf9F44CXUQmhMJ8fMby8/Mu0F//80G++W+vs6+rN+qmiIhMKt4F+ra9Xaz63Tbau/qiboqITFIdHR1873vfG9NnL7vsMjo6OkZd/2tf+xp33nnnmL6r1LwL9CD8K8ShB3OISHFHCvRkMnnEz65Zs4a6urqJaNaE8y7Qs+NK6XTEDRGRSeuWW25h+/btLF68mC996UusXbuWiy++mCuuuIIFCxYA8NGPfpTzzz+fhQsXcu+99w58tqmpiX379rFz507mz5/PjTfeyMKFC7n00kvp7u4+4vdu3LiRpUuXsmjRIj72sY9x4MABAFatWsWCBQtYtGgR11xzDQC///3vWbx4MYsXL+bcc8/l4MGD495u705bzPbQ03p0nogXbn/sFV59+92SrnPBScfx1f+ycNjld9xxB5s3b2bjxo0ArF27lg0bNrB58+aB0wAfeOABZs6cSXd3NxdccAFXXXUV9fX1Q9azdetWHnnkEe677z6uvvpqHn30Ua677rphv/f666/nO9/5DsuXL+e2227j9ttv51vf+hZ33HEHO3bsoLKycmA458477+Suu+5i2bJldHV1UVVVNd4fi3899CDsoSvPReRoLFmyZMg53atWreKcc85h6dKl7Nq1i61btxZ8Zu7cuSxevBiA888/n507dw67/s7OTjo6Oli+fDkAN9xwA88++ywAixYt4hOf+AQPP/ww8XimH71s2TK++MUvsmrVKjo6OgbKx8O/Hnq4C1IPXcQPR+pJH0vTp08fmF67di1PPfUUzz33HNXV1VxyySVFz/murKwcmI7FYiMOuQznt7/9Lc8++yyPPfYYX//619m0aRO33HILl19+OWvWrGHZsmU88cQTnHXWWWNaf5Z3PfSBMXQFuogMo7a29ohj0p2dnRx//PFUV1fz2muv8fzzz4/7O2fMmMHxxx/PunXrAPjxj3/M8uXLSafT7Nq1iw996EN84xvfoLOzk66uLrZv387ZZ5/Nl7/8ZS644AJee+21cbfBvx76QKBH3BARmbTq6+tZtmwZ73vf+1i5ciWXX375kOUrVqzgnnvuYf78+bz3ve9l6dKlJfneBx98kJtvvpnDhw8zb948fvjDH5JKpbjuuuvo7OzEOcfnPvc56urq+MpXvsIzzzxDEAQsXLiQlStXjvv7zUXU021ubnZjecDFuq1tfPL+F/jlzRfR3DRzAlomIuO1ZcsW5s+fH3UzvFfs52hmLzrnmovV927IRT10EZHivAt002mLIiJFeRfogQ6KinghquHccjGWn5+3ga7fFZHJq6qqivb2doX6GGXvh360Fxt5eJZL5l09dJHJa86cObS2ttLW1hZ1U7yVfWLR0fAu0E0HRUUmvUQicVRP2pHS8HDIJfOuHrqIyFAeBnp2DF2BLiKSy9tA1+1zRUSGGjHQzewUM3vGzF41s1fM7PNF6piZrTKzbWb2spmdNzHN1XnoIiLDGc1B0STwD865DWZWC7xoZk86517NqbMSOCN8XQjcHb6XnK4UFREpbsQeunPuHefchnD6ILAFODmv2pXAQy7jeaDOzGaXvLUM3j5XY+giIkMd1Ri6mTUB5wLr8xadDOzKmW+lMPQxs5vMrMXMWsZ6fqp66CIixY060M2sBngU+IJzbkzPk3LO3euca3bONTc2No5lFTptUURkGKMKdDNLkAnznzjnflWkym7glJz5OWFZyekBFyIixY3mLBcD7ge2OOe+OUy11cD14dkuS4FO59w7JWznAN3LRUSkuNGc5bIM+CSwycw2hmX/CzgVwDl3D7AGuAzYBhwGPlX6pmaEIy7qoYuI5Bkx0J1z/85gjg5XxwGfKVWjjkQHRUVEivPuStHshUU6bVFEZCjvAj0INIYuIlKMf4Gu0xZFRIryMNA1hi4iUox3ga6bc4mIFOddoOt+6CIixXkb6BpyEREZysNAz7xryEVEZCjvAl0PiRYRKc67QA90YZGISFEeBrrutigiUozHgR5xQ0REJhnvAl3noYuIFOddoOt+6CIixXkY6Jn3tMZcRESG8DDQNYYuIlKMd4GuMXQRkeI8DHTDTOehi4jk8y7QITPsoiEXEZGhPA10DbmIiOTzMtBNPXQRkQJeBnqgMXQRkQKeBrppyEVEJI/HgR51K0REJhcvA93QQVERkXx+BrrpXi4iIvm8DPQgMB0UFRHJ42egawxdRKSAp4GuMXQRkXxeBrouLBIRKeRloOvCIhGRQp4Gui4sEhHJ53GgR90KEZHJxctANx0UFREp4GWgB2a6sEhEJM+IgW5mD5jZXjPbPMzyS8ys08w2hq/bSt/MoXTaoohIofgo6vwI+C7w0BHqrHPO/WVJWjQKGkMXESk0Yg/dOfcssP8YtGXUNIYuIlKoVGPoF5nZS2b2/8xs4XCVzOwmM2sxs5a2trYxf1lmDF2BLiKSqxSBvgE4zTl3DvAd4F+Gq+icu9c51+yca25sbBzzFwZmpNNj/riISFkad6A75951znWF02uAhJk1jLtlR6AhFxGRQuMOdDM70cwsnF4SrrN9vOs9Eh0UFREpNOJZLmb2CHAJ0GBmrcBXgQSAc+4e4OPA35lZEugGrnETPMAdBLqXi4hIvhED3Tl37QjLv0vmtMZjRvdyEREp5OWVorp9rohIIS8DXVeKiogU8jTQdS8XEZF8nga6eugiIvm8DHTTQVERkQJeBnqmhx51K0REJhdPA133chERyedloJt66CIiBbwMdF1YJCJSyN9AVxddRGQILwM9FuhKURGRfF4GemCQUqKLiAzhaaBrDF1EJJ+XgR4LTD10EZE8XgZ6EKiHLiKSz89A1+1zRUQKeBnoMR0UFREp4GWgBxpDFxEp4GWgx3SWi4hIAT8DXQdFRUQKeBnoZkYqHXUrREQmFy8DPRboiUUiIvn8DHTTQVERkXxeBrouLBIRKeRnoOv2uSIiBbwM9FhgpNRDFxEZwstAz/TQo26FiMjk4mWgxwLUQxcRyeNloOt+6CIihbwNdOfAKdRFRAZ4GeixwADdcVFEJJffga4euojIAC8DPbBMoCvPRUQGeRromXcNuYiIDPIy0DXkIiJSyMtAzw656PJ/EZFBIwa6mT1gZnvNbPMwy83MVpnZNjN72czOK30zh9JZLiIihUbTQ/8RsOIIy1cCZ4Svm4C7x9+sI8uOoSvPRUQGjRjozrlngf1HqHIl8JDLeB6oM7PZpWpgMUGY6LpaVERkUCnG0E8GduXMt4ZlBczsJjNrMbOWtra2MX9hzDTkIiKS75geFHXO3euca3bONTc2No55PYHG0EVECpQi0HcDp+TMzwnLJkxMFxaJiBQoRaCvBq4Pz3ZZCnQ6594pwXqHFYSt1nnoIiKD4iNVMLNHgEuABjNrBb4KJACcc/cAa4DLgG3AYeBTE9XYrEBj6CIiBUYMdOfctSMsd8BnStaiUYjpLBcRkQJeXimaHUNXoIuIDPIy0E1DLiIiBbwM9IEhFz0oWkRkgKeBnnnXWS4iIoO8DHSd5SIiUsjrQNdDokVEBnkZ6Lp9rohIIS8DfWDIRT10EZEBXga6znIRESnkaaBn3nVhkYjIIC8D3TTkIiJSwMtAj+kh0SIiBfwMdJ3lIiJSwMtAD3RzLhGRAn4G+sBB0WjbISIymXgZ6HpItIhIIS8DPdADLkRECngZ6Oqhi4gU8jPQB3roETdERGQS8TLQg4HTFnXtv4hIlpeBXhFe+9+XVKCLiGR5GejTKmIA9PQr0EVEsrwM9Kp4ptnd/amIWyIiMnl4GejxWEAiZgp0EZEcXgY6QFUiRo8CXURkgAJdRKRMeBvo0xIxuvsU6CIiWV4Hus5yEREZ5G2gVyUCHRQVEcnhcaBrDF1EJJe3gT6tQoEuIpLL20Cvisc05CIiksPbQM/00HVQVEQky9tA10FREZGhPA50jaGLiOTyNtB1YZGIyFCjCnQzW2Fmr5vZNjO7pcjyvzazNjPbGL7+pvRNHaq2KkEy7Tjcl5zorxIR8UJ8pApmFgPuAj4CtAJ/NLPVzrlX86r+3Dn32QloY1H10ysAaO/qo3rmiJshIlL2RtNDXwJsc8696ZzrA34GXDmxzRpZfU0m0Pcf6ou4JSIik8NoAv1kYFfOfGtYlu8qM3vZzH5pZqcUW5GZ3WRmLWbW0tbWNobmDqqvqQSg/VDvuNYjIlIuSnVQ9DGgyTm3CHgSeLBYJefcvc65Zudcc2Nj47i+MDvksq9LPXQRERhdoO8Gcnvcc8KyAc65dudctqv8A+D80jRveNkhl3YFuogIMLpA/yNwhpnNNbMK4BpgdW4FM5udM3sFsKV0TSyuuiLOtESM/RpyEREBRnGWi3MuaWafBZ4AYsADzrlXzOyfgBbn3Grgc2Z2BZAE9gN/PYFtHnDijCre7ug5Fl8lIjLpjep8P+fcGmBNXtltOdO3AreWtmkjm9swne1tXcf6a0VEJiVvrxQFeE/jdHbsO0Q67aJuiohI5LwO9HmNNfQm0+zu6I66KSIikfM60M86sRaAzbs7I26JiEj0vA70hSfNYFoixvod+6NuiohI5LwO9Ip4wHmn1fGH7fuiboqISOS8DnSAv5g/izf2dLFt78GomyIiEinvA/3ys2cTC4yfrt81cmURkTLmfaCfcFwVH118Mj9Z/xZ7D+oiIxGZurwPdIC///DpJNOOVU9vjbopIiKRKYtAb2qYzvUXncbDz/+Jta/vjbo5IiKRKItAB/jyirN476xa/sfPN+p2ACIyJZVNoFclYnz/k+cTC4zr73+Bt9oPRd0kEZFjqmwCHTJDLz/61BIO9yW56u7ndAWpiEwpZRXoAO87eQa/uPn9VMYDrv7+c/xm4+6RPyQiUgbKLtABTj+hhl/99/ez8KTj+PzPNvKPv95ET38q6maJiEyosgx0gFnHVfHTG5fyt8vn8ZP1f+LyVevY8KcDUTdLRGTClG2gAyRiAbeunM/Dn76Qnv40H7/7D/zvx1+ls7s/6qaJiJRcWQd61gfOaOBfv3Ax1yw5lQf+YweX/N9neOi5nSRT6aibJiJSMlMi0AFqqxL8n4+dzeN//wHOOvE4bvvNK6z49jqe3rIH5/TEIxHx35QJ9KyFJ83gpzdeyH3XN5NKOz79YAsrv72OX7TsojepA6ci4i+Lqnfa3NzsWlpaIvnurL5kmtUvvc0P1r3Ja38+SGNtJTdcdBpXN5/CCcdVRdo2EZFizOxF51xz0WVTOdCznHP8+7Z93LduB8++0UZg8MEzG/mv583h0gWzqErEom6iiAigQD8q29u6+NWGVn69YTdvd/ZQWxXnIwtm8ZH5s7j4zEZqKuNRN1FEpjAF+hik047n32zn0Q27eWrLHjq7+6mIBVz0nnr+YsEsLj69gdPqqzGzqJsqIlPIkQJd3c1hBIHx/tMbeP/pDSRTaVreOsDTW/bw5Kt7+Mq/bAbgxOOquHDeTC6cW8+F82Yyr2G6Al5EIqMe+lFyzvHmvkM8t72d599sZ/2O/bQd7AWgsbaS806tY9GcOhafUsfZc2ZwXFUi4haLSDlRD72EzIz3NNbwnsYarlt6Gs45duw7xPod+1n/Zjsbd3XwxCt7BurPa5jO/NnHceasWs6cVcOZJ9Zy2sxq4rEpd8aoiEwwBfo4mRnzGmuY11jDtUtOBaDjcB8vt3ayaXcnL+3qYPPbnazZ/A7ZP4Yq4kG4U5hOU/10mhqmM7ehmtPqp1M/vULDNiIyJgr0CVBXXcEHz2zkg2c2DpR196XYtreL1/cc5I09B3n9zwd5ubWTNZveIZ0z6lVbGeeUmdWcVFfFSXXTOKluGrNnVHFyOH1CbaV69yJSlAL9GJlWEePsOTM4e86MIeV9yTStBw6zs/0QO/dl3lsPdNN6oJsXduzn3Z7kkPqxwJhVW8nsMNwbasJXbcXAdGM4X12hf16RqUT/4yNWEQ8GhmyK6epN8k5HN7s7unm7o4d3OrPT3byx5yB/2N4+7N0jqytiYchXUF9TyfHVCeqqK5gxLcHx1RXUVSeom5Ypq6tOUFedYFoipiEfEU8p0Ce5mso4Z8yq5YxZtcPW6UumaT/Uy76Dfezr6qWtq5d9XYPz+7p62bX/MJta+zlwuI/e5PB3mayIB8yYlqCmMk5NZZzplbGc6fjQ6api5TFqKxNMr4xpaEjkGFOgl4GKeMDsGdOYPWPaqOr39KfoONxPR3cfBw7109ndR8fhfg6EZe9299PVm6Krp59DvSne7uihqzfJod4kXb3JI+4QclXGA2qrBgO/YIdQGaMmDP+ayjhViRiV8YCKeEBlPEZlIqAynM6Uha+wXjww/TUhkkOBPgVVJWKcOCPGiTPGdgOy/lSaQ71JDvYkOdSXHJzuTdHVm9kZZMN/YEfQk5ne824Pb/YmMzuM3n56+sd+T/rAGAz/eBDuAGJUxIIhO4PsTqBYeUXeTmLIZ3LWOWRHk1MeC7RDkclDgS5HLRELwnH3inGvK5lKc6gvRVdvkr5kmt5kit7+NL3hdKbsSOVpevtT9CbTQ+sm0/T2p+no7qe3v8h6Upn64xULbOiOINxhVOTvGMK/MhKxgETMiMeMeJCZjoXv8SAgHrOB6eyy/LLBegGxwArK4kH4HrPM8rxlMf1lU7ZGFehmtgL4NhADfuCcuyNveSXwEHA+0A78lXNuZ2mbKuUoHguYMS0zbn+spdOOvlSRnUT/4E5h5B3KYHlf7s4kp15Xb5Le/jQ9yRTJlKM/lSaZDt9TjlTa0Z9Ocywv2k7khX08FpAIjFi4cwgM4kFmh5F9xQMjCN9zywbrZHYYgYXlMSNmw9UdXF/MMp/JTGduuxGEnwuMzLLsfFgWM8upl1fHwjpBTh0zLCzLXe9A/YF2kNcmctY5uN7JukMcMdDNLAbcBXwEaAX+aGarnXOv5lT7NHDAOXe6mV0DfAP4q4losEipBIFRFcTC2yNHf4uGVHow7JOpNP0pRzKdCf38sv5UZj6zMyisP7CecFkyldlpJMPP9afd4PflrDMdfiYVvjLTaVIOUjnr7u4frJOtN+xnc+s4R3/K/yeEWXaHEIb+4HTOziRnJzOwMwl3LNcuOZW/uXheyds1mh76EmCbc+7NzIbYz4ArgdxAvxL4Wjj9S+C7ZmZOz3YTGbVM73Fq3Hs/nXakXCbo09n3NJlpl9k5pB0509m65ExnPpNy2elh6jhHKlz38OsNy9M563IM1nEOFy5PpR3OZdufub9TKp1Xx7nB8myd8DvSaUdDTeWE/FxHE+gnA7ty5luBC4er45xLmlknUA/sK0UjRaS8BIERYOjZMaV1TE8UNrObzKzFzFra2tqO5VeLiJS90QT6buCUnPk5YVnROmYWB2aQOTg6hHPuXudcs3OuubGxMX+xiIiMw2gC/Y/AGWY218wqgGuA1Xl1VgM3hNMfB36n8XMRkWNrxDH0cEz8s8ATZE5bfMA594qZ/RPQ4pxbDdwP/NjMtgH7yYS+iIgcQ6M6D905twZYk1d2W850D/DfSts0ERE5Grp7kohImVCgi4iUCQW6iEiZsKhORjGzNuCtMX68gal30ZK2eWrQNk8N49nm05xzRc/7jizQx8PMWpxzzVG341jSNk8N2uapYaK2WUMuIiJlQoEuIlImfA30e6NuQAS0zVODtnlqmJBt9nIMXURECvnaQxcRkTwKdBGRMuFdoJvZCjN73cy2mdktUbenVMzsATPba2abc8pmmtmTZrY1fD8+LDczWxX+DF42s/Oia/nYmdkpZvaMmb1qZq+Y2efD8rLdbjOrMrMXzOylcJtvD8vnmtn6cNt+Ht7ZFDOrDOe3hcubomz/WJlZzMz+08weD+fLensBzGynmW0ys41m1hKWTejvtleBnvN805XAAuBaM1sQbatK5kfAiryyW4CnnXNnAE+H85DZ/jPC103A3ceojaWWBP7BObcAWAp8Jvz3LOft7gU+7Jw7B1gMrDCzpWSew/vPzrnTgQNkntMLOc/rBf45rOejzwNbcubLfXuzPuScW5xzzvnE/m678Nl3PryAi4AncuZvBW6Nul0l3L4mYHPO/OvA7HB6NvB6OP194Npi9Xx+Ab8h8zDyKbHdQDWwgcwjHfcB8bB84PeczG2rLwqn42E9i7rtR7mdc8Lw+jDwOGDlvL05270TaMgrm9Dfba966BR/vunJEbXlWJjlnHsnnP4zMCucLrufQ/in9bnAesp8u8Phh43AXuBJYDvQ4ZxLhlVyt2vI83qB7PN6ffIt4H8C6XC+nvLe3iwH/JuZvWhmN4VlE/q7Par7oUv0nHPOzMryHFMzqwEeBb7gnHvXzAaWleN2O+dSwGIzqwN+DZwVcZMmjJn9JbDXOfeimV0SdXuOsQ8453ab2QnAk2b2Wu7Cifjd9q2HPprnm5aTPWY2GyB83xuWl83PwcwSZML8J865X4XFZb/dAM65DuAZMkMOdeHzeGHodo3qeb2T2DLgCjPbCfyMzLDLtynf7R3gnNsdvu8ls+NewgT/bvsW6KN5vmk5yX1W6w1kxpiz5deHR8aXAp05f8Z5wzJd8fuBLc65b+YsKtvtNrPGsGeOmU0jc8xgC5lg/3hYLX+bvX1er3PuVufcHOdcE5n/r79zzn2CMt3eLDObbma12WngUmAzE/27HfWBgzEcaLgMeIPMuOM/Rt2eEm7XI8A7QD+Z8bNPkxk7fBrYCjwFzAzrGpmzfbYDm4DmqNs/xm3+AJlxxpeBjeHrsnLebmAR8J/hNm8GbgvL5wEvANuAXwCVYXlVOL8tXD4v6m0Yx7ZfAjw+FbY33L6Xwtcr2aya6N9tXfovIlImfBtyERGRYSjQRUTKhAJdRKRMKNBFRMqEAl1EpEwo0EVEyoQCXUSkTPx/v1/qZp4DcP8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcjklEQVR4nO3df5RcZZ3n8fe3fvRPOkmn08SQDnQcgxCRgIkRJYuubJyADjgwCpzxjLquObs7sLjjuie4igw6Z1zPuO56FmeNLkecdckwomNwc2QEw+ARUBoE8ov8EILpEJJO6CSd9I/qqvvdP+p2dXV3Jd1pqlJ5Kp/XOXWq7nOfrvretv3w5LnPvWXujoiIhC9R7QJERKQ8FOgiIjVCgS4iUiMU6CIiNUKBLiJSI1LV+uA5c+Z4Z2dntT5eRCRIzzzzzEF3by+1r2qB3tnZSVdXV7U+XkQkSGb2yon2acpFRKRGKNBFRGqEAl1EpEYo0EVEasSkgW5m95rZATPbfIL9ZmbfNLNdZvaCmb2j/GWKiMhkpjJC/x6w6iT7rwEWxY/VwN++8bJERORUTRro7v448PpJulwPfN/zngJmmdm8chUoIiJTU4516POBPUXb3XHbvvEdzWw1+VE8559/fhk+WkQE3J1c5GQjJ3InnUwQxW3DuZF9Uf45N7ZvLip6uBONeU3+uag9/zNM6Osl2gvvH7dFDrnIufric7m0Y1bZfw+n9cIid18LrAVYtmyZbsQuchq450NtOBeRyUZk4uehbP55ODfaVmjPje5LmpFKGpGTDyofF1ZFIRe509M3RC5yUkkjk41wwB1yUUQ2lw/SfNBGhWAdv52N+468/3AUkcuN7HOycd/hojAOSXtL/Rkb6HuBBUXbHXGbyFnF3RmKQzCbc44NZTmeyTI0XByiOQaHI/ozOYZz+b4GHBvKkclGZKOI4Vw+sPJB64V+fYNZjg4MxyNGSo4Cs8XBnY0Yin/2dH6PTXNdkvp0kuFsRF0qgRmYGamEkUwY6WSCZGJ0O5VMFF7XpxM0JfLbqUT+PyTJCdtGKm5LJo10YvT9Usn85w1nIxKJE3/mmIeNvk4UbScK7fn6S7UnxrUlzEgkGO1b4v0S8e+jEsoR6OuBW81sHfAu4Ii7T5huETnd3J1MLmIwEzEwnOP14xmyUcTBY0MM5/LhOzicYygbMVT0PFi0PTjuuWRb/DOZbFSWuuuSCVJJoy6VIJ1MkE4Y6VSCc+pTzGxMjwaHMTYo4sCqSyaoSxU9konSbalx7ckE6fi5Pm4bmbrI5nw0rOKQShQ/F17n669UYMnJTRroZnY/8D5gjpl1A18C0gDu/r+ADcC1wC6gH/hkpYqV2hNFzrFMlnQiQSYb0dufYWA4B8CBviEGMlkGhnMMZCL6M1n6MzmOZ7L0D40+9w/n6B/K7xsYznF8KMtAJt8+nX+KN6QT1KeShef6VIKGdP65MZ1kVmOa+nSChlSS+pE+RX1HArm5PsU59al8QKZHw7OxLkljOkk67gfQUp8mHY8+FYYyXZMGurvfMsl+B/68bBVJMNyd/kyOo4PDHB3IcnRwmCP9wxwdHCYXOQf6hhjO5acXjg7k20emDfqGshjw2pFBjmdyp/S5dakEzXVJmupSNNcnaaxL0ZROct6sdOF1U32SprhPQzofoDMaU6STCc5tqSedTIwGdno0sDW6lJBV7W6LcubIRc7BY0PsPnicdCrBwb4h9vcN5UM4DuJCYBfa8sGcncIIuDGdpKUhxYzGNDMaUsxqqqOjtYmhbI5/said82Y1kI2cumSC1qY66tMJ3GHezAYa41BuHAnpdJJUUhc4i5SiQK9BA5kcvf0Zdh44xv4jg3EgD3N4YJjD/fnnI/2ZwvbRweETnjSrSyWYGQfxjMY0rU11dLY1M6MxxYyGNDMa0/H+dKFtZmOabOS0t9RzTn2KZEIjXpHTQYEekChy9vcN8urhAXr6hjjQN8Tug/0cGxrm0LEMrx4ZZN+RAQ73D0/4WTOY2ZhmVmOamU11zGqqo3NOc2G7tSnNgtYmkgljdnMdb5rZwMzGNA3pZBWOVESmQ4F+Buru7ee5PYfZ8/oAe3r72fN6P929A+ztHSCTG7uSYmRueHZzPefNbGDpBbOYN7ORtub8tMYFbU3MbEpzTl2KhEbKIjVNgV4Fg/ESulcO9bN131G2vHqEra8epadviL6h7Jjlb61NaRbMbmLxvBl84G1zWdDaxPzWRs5tqaf9nHrmnFOvoBYRQIF+Whw4Osgzr/TmH7/vZfPeIwznRietz22pZ/F5M1jW2UpzfYo5zfW85y1tnD+7iZaGdBUrF5GQKNDLLJuL2PLqUbbv7+NXuw7yzCu9dPcOAFCfSrCkYxb/esVCLpjdTEdrIxfPm0F7S32VqxaRWqBAf4My2Yhfv3yIF7qP8MudPWx/rY/e+KRke0s9yztn88krF7L0glYWz5tBXUpL7kSkMhTo0/Dia0f55Y6DbNx+gOf3HC5cGLPo3HO4+uK5XHVhOxfOPYe3zm3RRSoictoo0KdgcDjHP+/o4acv7OOJXQc5dDwD5AP8xqUdXLWonXe9ebbmu0WkqhToJ/H8nsP83VOv8PDm1+gbytLalOb9F83lsgUz+cDb3sTcGQ3VLlFEpECBPs5QNseGTfv43hOv8PyewzTXJbn27fP40JLzeM8ftJHWZecicoZSoMf2Hx3kvid280DXHg4ey/DmOc3c9UeLuXFph6ZSRCQIZ32g//5QP//n169w3xO7Gc5FvP+ic/mzd3ey4i1zdMGOiATlrA30Q8eGuPdXL/Ptf36JnDt/fPl8PnP1hZzf1lTt0kREpuWsDPTtr/Xx8Xt/w2tHB7nh8vl89g/fyvxZjdUuS0TkDTmrAn0om+NrP9vO957YzazGND/69+/hHee3VrssEZGyOGsCvadviL944Dl+ufMgtyw/n8/94VuZ3VxX7bJERMrmrAj0XQf6+Pi9T3Pw2BBfveHt3Lz8/GqXJCJSdjUd6O7Ohk2vccePXqAuleQf/u27ubRjVrXLEhGpiJoN9ChyvvqzF1n7+Eu8ff5MvvWn72DBbK1gEZHaVZOBPpyLWP39LjZu7+GW5efz5evfpi8WFpGaV3OB7u6seXATG7f38MUPLeaT7+nUBUIiclaouUD/+j/t4MFnu/nMv1rEp1YsrHY5IiKnTU3NQzy6bT//c+Mubn7nAm6/elG1yxEROa1qJtCP9A9zx482cdGbWrj7+kv0xRIictaZUqCb2Soz225mu8xsTYn9F5jZo2b2gpk9ZmYd5S/15O7+6VYOHc/wNx9Zoq95E5Gz0qTJZ2ZJ4B7gGmAxcIuZLR7X7W+A77v7pcDdwF+Xu9CT2bj9AA8+282/e+8fcMn8mafzo0VEzhhTGcouB3a5+0vungHWAdeP67MY+EX8emOJ/RUTRc5fb9jGm+c0c9vVbzldHysicsaZSqDPB/YUbXfHbcWeB26IX/8x0GJmbePfyMxWm1mXmXX19PRMp94JnnzpEDv2H+O2q99CfSpZlvcUEQlRuSab/xPwXjP7LfBeYC+QG9/J3de6+zJ3X9be3l6WD/5/m/bRVJfkmkvmleX9RERCNZV16HuBBUXbHXFbgbu/SjxCN7NzgBvd/XC5ijyRbC7i4c2v8f6LzqUhrdG5iJzdpjJCfxpYZGYLzawOuBlYX9zBzOaY2ch73QHcW94yS/vNy69z6HiGD12q0bmIyKSB7u5Z4FbgYWAb8IC7bzGzu83surjb+4DtZrYDmAv8VYXqHeM3u1/HDK66sDzTNyIiIZvSpf/uvgHYMK7tzqLXPwR+WN7SJvfivj4WtjXTVFdzdzAQETllQV+Bs31/H299U0u1yxAROSMEG+hD2Ry7Dx1n0VwFuogIBBzoPX1DuMP8WQ3VLkVE5IwQbKDvPzoEwLktCnQREQg40Hv6BgFob6mvciUiImeGYAP9QF88Qp+hQBcRgZAD/egQCYO2ZgW6iAgEHOg9fUPMOaeepL4vVEQECDjQjw1laWnQBUUiIiOCDfTjmayuEBURKRJsoPdncjTV6Q6LIiIjgg30AQW6iMgYwQZ6v6ZcRETGCDbQBzI5GjVCFxEpCDbQ+4c15SIiUizcQNcIXURkjCADPZuLyGQjmtKaQxcRGRFkoPcP5wA05SIiUiTIQB/IxIFer0AXERkRZKD3ZzRCFxEZL9BAzwLQqDl0EZGCIAN9QCN0EZEJggz0TC4CIJ0MsnwRkYoIMhGjfJ6TSupe6CIiI4IM9Gyc6PpyCxGRUUEGei5yAJKmQBcRGTGlQDezVWa23cx2mdmaEvvPN7ONZvZbM3vBzK4tf6mjsiOBrhG6iEjBpIFuZkngHuAaYDFwi5ktHtftC8AD7n45cDPwrXIXWiyKA11z6CIio6YyQl8O7HL3l9w9A6wDrh/Xx4EZ8euZwKvlK3GikRF6SiN0EZGCqQT6fGBP0XZ33FbsLuBjZtYNbABuK/VGZrbazLrMrKunp2ca5eaNzKEnNIcuIlJQrpOitwDfc/cO4Frg78xswnu7+1p3X+buy9rb26f9YaMj9CDP6YqIVMRUEnEvsKBouyNuK/Yp4AEAd38SaADmlKPAUkbm0JOaQxcRKZhKoD8NLDKzhWZWR/6k5/pxfX4PXA1gZheTD/Tpz6lMQnPoIiITTRro7p4FbgUeBraRX82yxczuNrPr4m6fBT5tZs8D9wOfcHevVNE5XVgkIjLBlG5X6O4byJ/sLG67s+j1VuDK8pZ2YlldWCQiMkGQZxVzmkMXEZkg6EDXHLqIyKggA12X/ouITBRkoOvmXCIiEwUZ6Bqhi4hMFGSgR5GTTBimEbqISEGQgZ6NA11EREYFGei5KNL8uYjIOEEGejZyLVkUERknyECPItdFRSIi4wQZ6Bqhi4hMFGSg53RSVERkgiADPRu5ToqKiIwTZKBrDl1EZKIgAz0/hx5k6SIiFRNkKmoOXURkoiADPasLi0REJggy0HORbswlIjJeoIEekdJJURGRMYIMdN2cS0RkoiADPad16CIiEwQZ6Bqhi4hMFGSgR5FrDl1EZJwgAz0/Qg+ydBGRigkyFXO626KIyARBBno2chI6KSoiMsaUAt3MVpnZdjPbZWZrSuz/hpk9Fz92mNnh8pc6KtIIXURkgtRkHcwsCdwDrAS6gafNbL27bx3p4+7/saj/bcDlFai1IBtFutuiiMg4UxmhLwd2uftL7p4B1gHXn6T/LcD95SjuRCJHUy4iIuNMJdDnA3uKtrvjtgnM7AJgIfCLE+xfbWZdZtbV09NzqrUWRO5oxkVEZKxynxS9Gfihu+dK7XT3te6+zN2Xtbe3T/tD8oGuRBcRKTaVQN8LLCja7ojbSrmZCk+3ALiD8lxEZKypBPrTwCIzW2hmdeRDe/34TmZ2EdAKPFneEidyB0OJLiJSbNJAd/cscCvwMLANeMDdt5jZ3WZ2XVHXm4F17u6VKXVMTZpDFxEZZ9JliwDuvgHYMK7tznHbd5WvrJPTKhcRkYmCvFI0ctccuojIOEEGugOmRBcRGSPMQNccuojIBEEGeqRliyIiEwQZ6K4Li0REJggy0LXKRURkokADveJL3UVEghNkoKMRuojIBEEGuu62KCIyUaCBrlUuIiLjBRnojla5iIiMF2SgRw662aKIyFhBBrpOioqITBRkoOukqIjIRMEGur7gQkRkrCAD3UEjdBGRcYILdHePv1NUiS4iUizAQM8/K89FRMYKL9DjZ61yEREZK7hAH7kxl+bQRUTGCjbQNYcuIjJWcIGuOXQRkdKCDXTNoYuIjBVcoBemXKpch4jImSa4QNcqFxGR0oIL9NGTolUuRETkDDOlQDezVWa23cx2mdmaE/T5qJltNbMtZvZ/y1vmqNGTokp0EZFiqck6mFkSuAdYCXQDT5vZenffWtRnEXAHcKW795rZuZUq2LUOXUSkpKmM0JcDu9z9JXfPAOuA68f1+TRwj7v3Arj7gfKWOSoaGaFX6gNERAI1lUCfD+wp2u6O24pdCFxoZr8ys6fMbFWpNzKz1WbWZWZdPT090yq4MELXEF1EZIxynRRNAYuA9wG3AN8xs1njO7n7Wndf5u7L2tvbp/VBkebQRURKmkqg7wUWFG13xG3FuoH17j7s7i8DO8gHfNm51qGLiJQ0lUB/GlhkZgvNrA64GVg/rs8/kh+dY2ZzyE/BvFTGOgu0Dl1EpLRJA93ds8CtwMPANuABd99iZneb2XVxt4eBQ2a2FdgIfM7dD1WiYN1tUUSktEmXLQK4+wZgw7i2O4teO/AX8aOiIt2cS0SkpOCuFHXdPldEpKQAAz3/rDl0EZGxggt03W1RRKS04AK9MEIPrnIRkcoKLhZHR+gao4uIFAsu0EfWoWsKXURkrPACvbAOXYkuIlIsuEDXOnQRkdKCC3QtWxQRKS24QNel/yIipQUb6FqJLiIyVnCBPjrlUt06RETONAEHuhJdRKRYcIFeuLBIeS4iMkZwga4vuBARKS24QNcIXUSktOACXfdDFxEpLcBAzz9rlYuIyFjBBXrh0n+tQxcRGSPAQNeVoiIipQQX6F64OZcSXUSkWICBrlUuIiKlhBfo8bPWoYuIjBVcoGsOXUSktAADPf+sAbqIyFjBBbouLBIRKW1KgW5mq8xsu5ntMrM1JfZ/wsx6zOy5+PFvyl9qnu62KCJSWmqyDmaWBO4BVgLdwNNmtt7dt47r+vfufmsFahyjcC+XSn+QiEhgpjJCXw7scveX3D0DrAOur2xZJ6YRuohIaVMJ9PnAnqLt7rhtvBvN7AUz+6GZLSj1Rma22sy6zKyrp6dnGuXqbosiIidSrpOiDwGd7n4p8HPgvlKd3H2tuy9z92Xt7e3T+iCtchERKW0qgb4XKB5xd8RtBe5+yN2H4s3vAkvLU14pI+vQlegiIsWmEuhPA4vMbKGZ1QE3A+uLO5jZvKLN64Bt5StxrEhz6CIiJU26ysXds2Z2K/AwkATudfctZnY30OXu64H/YGbXAVngdeATlSpYc+giIqVNGugA7r4B2DCu7c6i13cAd5S3tBPVkn/Wpf8iImMFd6XoyAhdK9FFRMYKLtBHaIQuIjJWcIE+erdFJbqISLHwAj3KPyvPRUTGCi7Q9QUXIiKlTWmVy5lEyxZFwjE8PEx3dzeDg4PVLiU4DQ0NdHR0kE6np/wzwQW67ocuEo7u7m5aWlro7OzU/2dPgbtz6NAhuru7Wbhw4ZR/LrwpF61DFwnG4OAgbW1tCvNTZGa0tbWd8r9sggt0XfovEhaF+fRM5/cWYKDrCy5EREoJLtAL14nqv/oiMonDhw/zrW99a1o/e+2113L48OEyV1RZ4QV64cKiKhciIme8kwV6Nps96c9u2LCBWbNmVaKsiglulUsUaZWLSIj+8qEtbH31aFnfc/F5M/jSH73thPvXrFnD7373Oy677DJWrlzJBz/4Qb74xS/S2trKiy++yI4dO/jwhz/Mnj17GBwc5Pbbb2f16tUAdHZ20tXVxbFjx7jmmmtYsWIFTzzxBPPnz+cnP/kJjY2NYz7roYce4itf+QqZTIa2tjZ+8IMfMHfuXI4dO8Ztt91GV1cXZsaXvvQlbrzxRn72s5/x+c9/nlwux5w5c3j00Uff8O8juEAfvbCoqmWISAC++tWvsnnzZp577jkAHnvsMZ599lk2b95cWA547733Mnv2bAYGBnjnO9/JjTfeSFtb25j32blzJ/fffz/f+c53+OhHP8qDDz7Ixz72sTF9VqxYwVNPPYWZ8d3vfpevfe1rfP3rX+fLX/4yM2fOZNOmTQD09vbS09PDpz/9aR5//HEWLlzI66+/XpbjDS7QC19Bp9OiIkE52Uj6dFq+fPmYtd3f/OY3+fGPfwzAnj172Llz54RAX7hwIZdddhkAS5cuZffu3RPet7u7m5tuuol9+/aRyWQKn/HII4+wbt26Qr/W1lYeeughrrrqqkKf2bNnl+XYgp1Dt+AqF5EzQXNzc+H1Y489xiOPPMKTTz7J888/z+WXX15y7Xd9fX3hdTKZLDn/ftttt3HrrbeyadMmvv3tb1fl6tjgYtG1Dl1EpqilpYW+vr4T7j9y5Aitra00NTXx4osv8tRTT037s44cOcL8+fMBuO+++wrtK1eu5J577ils9/b2csUVV/D444/z8ssvA5RtyiW4QNc6dBGZqra2Nq688kouueQSPve5z03Yv2rVKrLZLBdffDFr1qzhiiuumPZn3XXXXXzkIx9h6dKlzJkzp9D+hS98gd7eXi655BKWLFnCxo0baW9vZ+3atdxwww0sWbKEm266adqfW8y88A1Ap9eyZcu8q6vrlH/u51v384+/3cvXP7qEhnSyApWJSLls27aNiy++uNplBKvU78/MnnH3ZaX6B3dSdOXiuaxcPLfaZYiInHGCm3IREZHSFOgiUlHVmtYN3XR+bwp0EamYhoYGDh06pFA/RSP3Q29oaDilnwtuDl1EwtHR0UF3dzc9PT3VLiU4I99YdCoU6CJSMel0+pS+cUfeGE25iIjUCAW6iEiNUKCLiNSIql0pamY9wCvT/PE5wMEylhMCHfPZQcd8dngjx3yBu7eX2lG1QH8jzKzrRJe+1iod89lBx3x2qNQxa8pFRKRGKNBFRGpEqIG+ttoFVIGO+eygYz47VOSYg5xDFxGRiUIdoYuIyDgKdBGRGhFcoJvZKjPbbma7zGxNtespFzO718wOmNnmorbZZvZzM9sZP7fG7WZm34x/By+Y2TuqV/n0mdkCM9toZlvNbIuZ3R631+xxm1mDmf3GzJ6Pj/kv4/aFZvbr+Nj+3szq4vb6eHtXvL+zmvVPl5klzey3ZvbTeLumjxfAzHab2SYze87MuuK2iv5tBxXoZpYE7gGuARYDt5jZ4upWVTbfA1aNa1sDPOrui4BH423IH/+i+LEa+NvTVGO5ZYHPuvti4Argz+P/PWv5uIeA97v7EuAyYJWZXQH8V+Ab7v4WoBf4VNz/U0Bv3P6NuF+Ibge2FW3X+vGO+JfuflnRmvPK/m27ezAP4N3Aw0XbdwB3VLuuMh5fJ7C5aHs7MC9+PQ/YHr/+NnBLqX4hP4CfACvPluMGmoBngXeRv2owFbcX/s6Bh4F3x69TcT+rdu2neJwdcXi9H/gp+e94r9njLTru3cCccW0V/dsOaoQOzAf2FG13x221aq6774tfvwaMfJlqzf0e4n9aXw78mho/7nj64TngAPBz4HfAYXfPxl2Kj6twzPH+I0Db6a34DfvvwH8Goni7jdo+3hEO/JOZPWNmq+O2iv5t637ogXB3N7OaXGNqZucADwKfcfejZlbYV4vH7e454DIzmwX8GLioyiVVjJl9CDjg7s+Y2fuqXc9ptsLd95rZucDPzezF4p2V+NsObYS+F1hQtN0Rt9Wq/WY2DyB+PhC318zvwczS5MP8B+7+o7i55o8bwN0PAxvJTznMMrORAVbxcRWOOd4/Ezh0mkt9I64ErjOz3cA68tMu/4PaPd4Cd98bPx8g/x/u5VT4bzu0QH8aWBSfIa8DbgbWV7mmSloPfDx+/XHyc8wj7X8Wnxm/AjhS9M+4YFh+KP6/gW3u/t+KdtXscZtZezwyx8wayZ8z2EY+2P8k7jb+mEd+F38C/MLjSdYQuPsd7t7h7p3k///6C3f/U2r0eEeYWbOZtYy8Bj4AbKbSf9vVPnEwjRMN1wI7yM87/pdq11PG47of2AcMk58/+xT5ucNHgZ3AI8DsuK+RX+3zO2ATsKza9U/zmFeQn2d8AXguflxby8cNXAr8Nj7mzcCdcfubgd8Au4B/AOrj9oZ4e1e8/83VPoY3cOzvA356NhxvfHzPx48tI1lV6b9tXfovIlIjQptyERGRE1Cgi4jUCAW6iEiNUKCLiNQIBbqISI1QoIuI1AgFuohIjfj/qT37LyfFPo4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}