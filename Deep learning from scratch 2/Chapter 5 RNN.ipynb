{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5IM8E7ECyDO"
      },
      "source": [
        "### **RNN**\n",
        "\n",
        "RNN은 순서가 있는 시계열 데이터를 처리하기 위해 고안된 신경망으로, 은닉계층을 가지고 있다. 이 은닉계층을 보통 h로 표시하는데, h<sub>t</sub>를 은닉 상태(hidden state) 혹은 은닉 상태 벡터(hidden state vector)라고 한다.\n",
        "\n",
        "#### **BPTT**\n",
        "\n",
        "BPTT는 RNN에서 수행하는 오차역전파로 직역하면 '시간 방향으로 펼친 신경망의 오차역전파법'이란 뜻이다. 그러나 일반적인 BPTT는 긴 시계열 데이터를 다룰 때, 문제가 발생하는데 시계열 데이터의 길이가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문이다. 또한, 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 것도 문제이다.\n",
        "\n",
        "#### **Truncated BPTT**\n",
        "\n",
        "Truncated BPTT는 이러한 BPTT의 문제를 해결하기 위해 고안된 것으로, 역전파의 연결을 적당한 길이로 잘라내, 잘라낸 신경망 단위로 학습을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y248kAxCysY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(grad) for grad in self.params]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
        "    h_next = np.tanh(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next\n",
        "\n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params\n",
        "    x, h_prev, h_next = self.cache\n",
        "\n",
        "    dt = dh_next * (1 - h_next ** 2)\n",
        "    db = np.sum(dt, axis=0)\n",
        "    dWh = np.matmul(h_prev.T, dt)\n",
        "    dh_prev = np.matmul(dt, Wh.T)\n",
        "    dWx = np.matmul(x.T, dt)\n",
        "    dx = np.matmul(dt, Wx.T)\n",
        "\n",
        "    self.grads[0][...] = dWx\n",
        "    self.grads[1][...] = dWh\n",
        "    self.grads[2][...] = db\n",
        "\n",
        "    return dx, dh_prev"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfnfaNrEE86H"
      },
      "source": [
        "TimeRNN은 RNN을 t개 모아놓은 것으로, Truncated BPTT를 구현하기 위해 구현한 클래스이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0JoMN3E-wG"
      },
      "source": [
        "class TimeRNN:\n",
        "  def __init__(self, Wx, Wh, b, stateful=False):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(param) for param in self.params]\n",
        "    self.layers = None\n",
        "\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful\n",
        "\n",
        "  def set_state(self, h):\n",
        "    self.h = h\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.h = None\n",
        "\n",
        "  def forward(self, xs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, D = xs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    self.layers = []\n",
        "    hs = np.empty((N, T, H), dtype=np.float32)\n",
        "\n",
        "    if not self.stateful or self.h is None:\n",
        "      self.h = np.zeros((N, H), dtype=np.float32)\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = RNN(*self.params)\n",
        "      self.h = layer.forward(xs[:, t, :], self.h)\n",
        "      hs[:, t, :] = self.h\n",
        "      self.layers.append(layer)\n",
        "    \n",
        "    return hs\n",
        "\n",
        "  def backward(self, hs):\n",
        "    Wx, Wh, b  = self.params\n",
        "    N, T, H = hs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    dxs = np.empty((N, T, D), dtype=np.float32)\n",
        "    dh = 0\n",
        "    grads = [0, 0, 0]\n",
        "    for t in reversed(range(T)):\n",
        "      layer = self.layers[t]\n",
        "      dx, dh = layer.backward(hs[:, t, :] + dh)\n",
        "      dxs[:, t, :] = dx\n",
        "\n",
        "      for i, grad in enumerate(layer.grads):\n",
        "        grads[i] += grad\n",
        "\n",
        "    for i, grad in enumerate(grads):\n",
        "      self.grads[i][...] = grad\n",
        "    self.dh = dh\n",
        "\n",
        "    return dxs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Nsk1joTsH6"
      },
      "source": [
        "### RNNLM\n",
        "\n",
        "지금까지 RNN 모듈과, 이를 여러개 모은 TimeRNN 모듈을 만들어보았다. 이러한 모듈을 활용해 RNNLM(RNN Language Model)을 만들어 보겠다. RNNLM은 지금까지 입력된 단어를 기억하고, 그것을 바탕으로 다음에 출현할 단어를 예측한다.\n",
        "\n",
        "먼저 시계열 데이터를 한꺼번에 처리하는 계층을 TimeEmbedding, TimeAffine 형태의 이름으로 구현해보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTNlvNcfUPI6"
      },
      "source": [
        "# softmax\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x -= x.max(axis=1, keepdims=True)\n",
        "    x = np.exp(x)\n",
        "    x /= x.sum(axis=1, keepdims=True)\n",
        "  elif x.ndim == 1:\n",
        "    x = x - np.max(x)\n",
        "    x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "  return x\n",
        "\n",
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "    np.add.at(dW, self.idx, dout)\n",
        "    return None\n",
        "\n",
        "# TimeEmbedding\n",
        "class TimeEmbedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.layers = None\n",
        "    self.W = W\n",
        "\n",
        "  def forward(self, xs):\n",
        "    N, T = xs.shape\n",
        "    V, D = self.W.shape\n",
        "\n",
        "    out = np.empty((N, T, D), dtype='f')\n",
        "    \n",
        "    self.layers = []\n",
        "    for t in range(T):\n",
        "      layer = Embedding(self.W)\n",
        "      out[:, t, :] = layer.forward(xs[:, t])\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    N, T, D = dout.shape\n",
        "\n",
        "    grad = 0\n",
        "    for t in reversed(range(T)):\n",
        "      layer = self.layers[t]\n",
        "      layer.backward(dout[:, t, :])\n",
        "      grad += layer.grads[0]\n",
        "\n",
        "    self.grads[0][...] = grad\n",
        "    return None\n",
        "\n",
        "# TimeAffine\n",
        "class TimeAffine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, T, D = x.shape\n",
        "    W, b = self.params\n",
        "\n",
        "    rs = x.reshape(N*T, -1)\n",
        "    out = np.dot(rs, W) + b\n",
        "    self.x = x\n",
        "    return out.reshape(N, T, -1)\n",
        "\n",
        "  def backward(self, dout):\n",
        "    x = self.x\n",
        "    N, T, D = x.shape\n",
        "    W, b = self.params\n",
        "\n",
        "    dout = dout.reshape(N*T, -1)\n",
        "    rx = x.reshape(N*T, -1)\n",
        "\n",
        "    db = np.sum(dout, axis=0)\n",
        "    dW = np.dot(rx.T, dout)\n",
        "    dx = np.dot(dout, W.T)\n",
        "    dx = dx.reshape(*x.shape)\n",
        "\n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "\n",
        "    return dx\n",
        "\n",
        "# TimeSoftmaxWithLoss\n",
        "class TimeSoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads= [], []\n",
        "    self.cache = None\n",
        "    self.ignore_label = -1\n",
        "\n",
        "  def forward(self, xs, ts):\n",
        "    N, T, V = xs.shape\n",
        "\n",
        "    # 원-핫 벡터로 라벨링 되었을 경우\n",
        "    if ts.ndim == 3:\n",
        "      ts = ts.argmax(axis=2)\n",
        "\n",
        "    mask = (ts != self.ignore_label)\n",
        "\n",
        "    xs = xs.reshape(N*T, -1)\n",
        "    ts = ts.reshape(N*T)\n",
        "    mask = mask.reshape(N*T)\n",
        "\n",
        "    ys = softmax(xs)\n",
        "    ls = np.log(ys[np.arange(N*T), ts])\n",
        "    ls *= mask\n",
        "    loss = -np.sum(ls)\n",
        "    loss /= np.sum(mask)\n",
        "\n",
        "    self.cache = (ts, ys, mask, (N, T, V))\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "    dx = ys\n",
        "    dx[np.arange(N*T), ts] -= 1\n",
        "    dx *= dout\n",
        "    dx /= mask.sum()\n",
        "    dx *= mask[:, np.newaxis]\n",
        "    \n",
        "    dx = dx.reshape((N, T, V))\n",
        "    return dx"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C87fqVFCrBn_"
      },
      "source": [
        "SimpleRnnlm 클래스를 만들어서 RNNLM 신경망을 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJj7AJXpkDV"
      },
      "source": [
        "class SimpleRnnlm:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "\n",
        "    # Xavier 초기화를 사용한 가중치 초기화\n",
        "    embed_W = (np.random.randn(V, D) / 100).astype(np.float32)\n",
        "    rnn_Wx = (np.random.randn(D, H) / np.sqrt(D)).astype(np.float32)\n",
        "    rnn_Wh = (np.random.randn(H, H) / np.sqrt(H)).astype(np.float32)\n",
        "\n",
        "    rnn_b = np.zeros(H).astype(np.float32)\n",
        "    affine_W = (np.random.randn(H, V) / np.sqrt(H)).astype(np.float32)\n",
        "    affine_b = np.zeros(V).astype(np.float32)\n",
        "\n",
        "    self.layers = [\n",
        "      TimeEmbedding(embed_W),\n",
        "      TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "      TimeAffine(affine_W, affine_b)\n",
        "    ]\n",
        "    self.loss_layer = TimeSoftmaxWithLoss()\n",
        "    self.rnn_layer = self.layers[1]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, ts):\n",
        "    for layer in self.layers:\n",
        "      xs = layer.forward(xs)\n",
        "    loss = self.loss_layer.forward(xs, ts)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.loss_layer.backward(dout)\n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout)\n",
        "    return dout\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.rnn_layer.reset_state()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2nnFACKw28a"
      },
      "source": [
        "### 언어모델의 평가\n",
        "\n",
        "#### 1. perplexity\n",
        "언어모델의 성능은 평가할때는 보통 perplexity를 자주 이용한다.\n",
        "\n",
        "예를들어, RNNLM 모델에 'you'를 넣으면 'say'가 나올 확률로 0.8이 계산되었다고 가정해보자. perplexity는 0.8의 역수로, 1.25가 된다.\n",
        "<br><br>\n",
        "이 perplexity는 분기 수(number of brahches)로 해석될 수 있는데, 직관적으로 보면 후보로 올 수 있는 단어의 개수가 'perplexity'개 만큼 있다는 것이다. 앞의 예에서는 'say'의 perplexity값이 1.25가 나왔으므로, 'you' 다음으로 나올 수 있는 강력한 후보는 1개라고 할 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 2. 입력이 여러개일 때의 perplexity\n",
        "\n",
        "만약 이번 RNNLM처럼 입력이 여러개면 다음과 같이 perplexity를 표현할 수 있다.\n",
        "\n",
        "> L = TimeSoftmaxWithLoss().forward(xs, ts) <br>\n",
        "perplexity = np.exp(L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJAWqmvIzuj6"
      },
      "source": [
        "RNNLM으로 학습해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yAoW36H4w3Qz",
        "outputId": "21f3fb96-88a7-4dba-ebf3-cd6cf3c2d3cf"
      },
      "source": [
        "# optimizer\n",
        "class SGD:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "        \n",
        "  def update(self, params, grads):\n",
        "    for i in range(len(params)):\n",
        "      params[i] -= self.lr * grads[i]\n",
        "\n",
        "# 학습 코드 시작\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100\n",
        "time_size = 5\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "data_size = len(xs)\n",
        "\n",
        "max_iters = data_size // (batch_size * time_size)\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []\n",
        "\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "# 각 미니배치에서 샘플을 읽기 시작 위치를 계산\n",
        "jump = (corpus_size - 1) // batch_size\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  for iter in range(max_iters):\n",
        "    batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "    batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "\n",
        "    for t in range(time_size): \n",
        "      for i, offset in enumerate(offsets):\n",
        "        batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "        batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "      time_idx += 1\n",
        "\n",
        "    loss = model.forward(batch_x, batch_t)\n",
        "    model.backward()\n",
        "    optimizer.update(model.params, model.grads)\n",
        "    total_loss += loss\n",
        "    loss_count += 1\n",
        "\n",
        "  ppl = np.exp(total_loss / loss_count)\n",
        "  print('| 에폭 %d | 퍼플렉시티 %.2f' % (epoch+1, ppl))\n",
        "  ppl_list.append(float(ppl))\n",
        "  total_loss, loss_count = 0, 0\n",
        "\n",
        "plt.plot(range(max_epoch), ppl_list)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('perplexity')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 | 퍼플렉시티 384.30\n",
            "| 에폭 2 | 퍼플렉시티 254.06\n",
            "| 에폭 3 | 퍼플렉시티 221.26\n",
            "| 에폭 4 | 퍼플렉시티 214.00\n",
            "| 에폭 5 | 퍼플렉시티 204.88\n",
            "| 에폭 6 | 퍼플렉시티 201.45\n",
            "| 에폭 7 | 퍼플렉시티 198.15\n",
            "| 에폭 8 | 퍼플렉시티 196.50\n",
            "| 에폭 9 | 퍼플렉시티 192.13\n",
            "| 에폭 10 | 퍼플렉시티 193.06\n",
            "| 에폭 11 | 퍼플렉시티 189.75\n",
            "| 에폭 12 | 퍼플렉시티 192.54\n",
            "| 에폭 13 | 퍼플렉시티 190.44\n",
            "| 에폭 14 | 퍼플렉시티 190.84\n",
            "| 에폭 15 | 퍼플렉시티 190.15\n",
            "| 에폭 16 | 퍼플렉시티 186.73\n",
            "| 에폭 17 | 퍼플렉시티 184.09\n",
            "| 에폭 18 | 퍼플렉시티 181.47\n",
            "| 에폭 19 | 퍼플렉시티 183.30\n",
            "| 에폭 20 | 퍼플렉시티 184.43\n",
            "| 에폭 21 | 퍼플렉시티 181.69\n",
            "| 에폭 22 | 퍼플렉시티 178.51\n",
            "| 에폭 23 | 퍼플렉시티 175.13\n",
            "| 에폭 24 | 퍼플렉시티 176.51\n",
            "| 에폭 25 | 퍼플렉시티 174.04\n",
            "| 에폭 26 | 퍼플렉시티 173.50\n",
            "| 에폭 27 | 퍼플렉시티 167.98\n",
            "| 에폭 28 | 퍼플렉시티 167.38\n",
            "| 에폭 29 | 퍼플렉시티 163.95\n",
            "| 에폭 30 | 퍼플렉시티 158.33\n",
            "| 에폭 31 | 퍼플렉시티 159.02\n",
            "| 에폭 32 | 퍼플렉시티 153.62\n",
            "| 에폭 33 | 퍼플렉시티 152.76\n",
            "| 에폭 34 | 퍼플렉시티 147.27\n",
            "| 에폭 35 | 퍼플렉시티 146.48\n",
            "| 에폭 36 | 퍼플렉시티 138.78\n",
            "| 에폭 37 | 퍼플렉시티 132.59\n",
            "| 에폭 38 | 퍼플렉시티 130.48\n",
            "| 에폭 39 | 퍼플렉시티 124.09\n",
            "| 에폭 40 | 퍼플렉시티 119.24\n",
            "| 에폭 41 | 퍼플렉시티 118.30\n",
            "| 에폭 42 | 퍼플렉시티 111.14\n",
            "| 에폭 43 | 퍼플렉시티 105.65\n",
            "| 에폭 44 | 퍼플렉시티 100.85\n",
            "| 에폭 45 | 퍼플렉시티 97.62\n",
            "| 에폭 46 | 퍼플렉시티 95.21\n",
            "| 에폭 47 | 퍼플렉시티 90.41\n",
            "| 에폭 48 | 퍼플렉시티 84.97\n",
            "| 에폭 49 | 퍼플렉시티 81.37\n",
            "| 에폭 50 | 퍼플렉시티 76.74\n",
            "| 에폭 51 | 퍼플렉시티 73.60\n",
            "| 에폭 52 | 퍼플렉시티 70.18\n",
            "| 에폭 53 | 퍼플렉시티 65.89\n",
            "| 에폭 54 | 퍼플렉시티 62.86\n",
            "| 에폭 55 | 퍼플렉시티 60.17\n",
            "| 에폭 56 | 퍼플렉시티 57.70\n",
            "| 에폭 57 | 퍼플렉시티 53.58\n",
            "| 에폭 58 | 퍼플렉시티 50.50\n",
            "| 에폭 59 | 퍼플렉시티 46.84\n",
            "| 에폭 60 | 퍼플렉시티 45.00\n",
            "| 에폭 61 | 퍼플렉시티 43.25\n",
            "| 에폭 62 | 퍼플렉시티 42.15\n",
            "| 에폭 63 | 퍼플렉시티 37.37\n",
            "| 에폭 64 | 퍼플렉시티 36.09\n",
            "| 에폭 65 | 퍼플렉시티 35.32\n",
            "| 에폭 66 | 퍼플렉시티 33.46\n",
            "| 에폭 67 | 퍼플렉시티 31.22\n",
            "| 에폭 68 | 퍼플렉시티 29.41\n",
            "| 에폭 69 | 퍼플렉시티 27.37\n",
            "| 에폭 70 | 퍼플렉시티 25.44\n",
            "| 에폭 71 | 퍼플렉시티 25.62\n",
            "| 에폭 72 | 퍼플렉시티 23.84\n",
            "| 에폭 73 | 퍼플렉시티 21.63\n",
            "| 에폭 74 | 퍼플렉시티 21.15\n",
            "| 에폭 75 | 퍼플렉시티 21.10\n",
            "| 에폭 76 | 퍼플렉시티 19.04\n",
            "| 에폭 77 | 퍼플렉시티 17.67\n",
            "| 에폭 78 | 퍼플렉시티 17.11\n",
            "| 에폭 79 | 퍼플렉시티 15.28\n",
            "| 에폭 80 | 퍼플렉시티 14.58\n",
            "| 에폭 81 | 퍼플렉시티 14.20\n",
            "| 에폭 82 | 퍼플렉시티 13.32\n",
            "| 에폭 83 | 퍼플렉시티 12.75\n",
            "| 에폭 84 | 퍼플렉시티 12.30\n",
            "| 에폭 85 | 퍼플렉시티 11.49\n",
            "| 에폭 86 | 퍼플렉시티 11.54\n",
            "| 에폭 87 | 퍼플렉시티 10.17\n",
            "| 에폭 88 | 퍼플렉시티 10.02\n",
            "| 에폭 89 | 퍼플렉시티 9.92\n",
            "| 에폭 90 | 퍼플렉시티 9.02\n",
            "| 에폭 91 | 퍼플렉시티 8.51\n",
            "| 에폭 92 | 퍼플렉시티 8.47\n",
            "| 에폭 93 | 퍼플렉시티 8.02\n",
            "| 에폭 94 | 퍼플렉시티 7.90\n",
            "| 에폭 95 | 퍼플렉시티 7.15\n",
            "| 에폭 96 | 퍼플렉시티 6.56\n",
            "| 에폭 97 | 퍼플렉시티 6.65\n",
            "| 에폭 98 | 퍼플렉시티 6.11\n",
            "| 에폭 99 | 퍼플렉시티 5.78\n",
            "| 에폭 100 | 퍼플렉시티 5.63\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}