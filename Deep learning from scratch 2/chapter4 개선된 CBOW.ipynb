{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7nJOivAUqPL"
      },
      "source": [
        "## word2vec 속도 개선\n",
        "\n",
        "이번 챕터에서는 word2vec의 속도를 개선하기 위해 2가지 기법을 도입한다.\n",
        "첫번째 개선으로는 Embedding이라는 새로운 계층을 도입하며, 두번째 개선으로 네거티브 샘플링이라는 새로운 손실함수를 도입한다.\n",
        "\n",
        "#### 1) Embedding 기법\n",
        "\n",
        "3챕터의 word2vec 구현에서는 단어를 원핫 표현으로 바꿨다. 그러나 원핫 표현은\n",
        "단어의 수가 많아질수록 행력의 크기가 커져서, 메모리 문제도 있고, 계산량도 증가하는 문제가 있다. 이러한 문제를 해결하기 위해 고안된 방식이 바로 Embedding 계층이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RtCMP2wSLfB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "\n",
        "    \"\"\"\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "      dW[word_id] += dout[i]\n",
        "    \"\"\"\n",
        "    np.add.at(dW, self.idx, dout)\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIgMn9ny2PHg"
      },
      "source": [
        "### 네거티브 샘플링\n",
        "\n",
        "네거티브 샘플링은 '다중 분류'를 '이진 분류'로 근사시키는 것을 말한다.\n",
        "예를 들어, 'you'와 'goodbye' 사이에 'say'가 와야한다면, 이전 챕터에서는\n",
        "단어수 중 'say' index에만 정답 레이블을 설정하여 다중 분류 기법을 사용하였다.\n",
        "\n",
        "그러나, 네거티브 샘플링에서는  'you'와 'goodbye'사이에 와야 하는 단어가 'say'입니까? 라는 질문에 대하여 '예' '아니오'로 답하는 이진 분류로 근사하여 문제를 해결한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P3hjHrkuLyz"
      },
      "source": [
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx)\n",
        "    out = np.sum(target_W * h, axis=1)\n",
        "    \n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "    \n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8eonMnP9jVt"
      },
      "source": [
        "위 신경망은 긍정적 예인 'say'에 대해서만 학습하게 된다. 신경망은 'say'에 대해서는 1에 가까운 값을 출력해야 하고, 'say'이외의 단어에서는 0에 가까운 값을 출력해야 한다.\n",
        "\n",
        "따라서, 긍정적 예인 'say'이외에도 부정적 예에 대하여 학습을 할 필요가 있다.\n",
        "네거티브 샘플링에서는 여기서 통계적 기법을 활용하는데, 각 단어의 출현 빈도수를 확률 분포로 나타낸 뒤, 높은 확률값을 가진 단어 위주로 선정하여 부정적 예를 5개~10개 정도 고른 후, 학습을 시행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvmMRVklCZMN",
        "outputId": "429ae059-96fc-40b1-90c8-3195c550bed1"
      },
      "source": [
        "import collections\n",
        "\n",
        "class UnigramSampler:\n",
        "  def __init__(self, corpus, power, sample_size):\n",
        "    self.corpus = corpus\n",
        "    self.sample_size = sample_size\n",
        "    self.vocab_size = None\n",
        "    self.word_p = None\n",
        "\n",
        "    counts = collections.Counter()\n",
        "    for word_id in corpus:\n",
        "      counts[word_id] += 1\n",
        "    \n",
        "    vocab_size = len(counts)\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.word_p = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "      self.word_p[i] = counts[i]\n",
        "\n",
        "    self.word_p = np.power(self.word_p, power)\n",
        "    self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "  def get_negative_sample(self, target):\n",
        "    batch_size = target.shape[0]\n",
        "\n",
        "    negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size), replace=True, p=self.word_p)\n",
        "    return negative_sample\n",
        "\n",
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1, 3, 0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 4]\n",
            " [4 4]\n",
            " [0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaZ9sDFMOC6y"
      },
      "source": [
        "### 네거티브 샘플링 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8ktor_mODi9"
      },
      "source": [
        "# 교차 엔트로피 함수\n",
        "def cross_entropy_error(y, t, eps=1e-7):\n",
        "  if y.ndim == 1:\n",
        "    y = y.reshape(1, y.size)\n",
        "    t = t.reshape(1, t.size)\n",
        "\n",
        "  if y.size == t.size:\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t * np.log(y[np.arange(batch_size), t] + eps)) / batch_size\n",
        "\n",
        "# 시그모이드 + 손실함수\n",
        "class SigmoidWithLoss:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = 1 / (1 + np.exp(-x)) # sigmoid\n",
        "\n",
        "    self.loss = cross_entropy_error(np.c_[1-self.y, self.y], self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) * dout / batch_size\n",
        "    return dx\n",
        "\n",
        "# 네거티브 샘플링\n",
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "    self.sample_size = sample_size\n",
        "    self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.embed_dot_layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, h, target):\n",
        "    batch_size = target.shape[0]\n",
        "    negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "    # 긍정적 예 순전파\n",
        "    score = self.embed_dot_layers[0].forward(h, target)\n",
        "    correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "    loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "    # 부정적 예 순전파\n",
        "    negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "    for i in range(self.sample_size):\n",
        "      negative_target =  negative_sample[:, i]\n",
        "      score = self.embed_dot_layers[i + 1].forward(h, negative_target)\n",
        "      loss += self.loss_layers[i + 1].forward(score, negative_label)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dh = 0\n",
        "    for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "      dscore = l0.backward(dout)\n",
        "      dh += l1.backward(dscore)\n",
        "\n",
        "    return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdHzUNb7Uqrn"
      },
      "source": [
        "### CBOW 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8_-efDdUpBI"
      },
      "source": [
        "class CBOW:\n",
        "  def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "    V, H = vocab_size, hidden_size\n",
        "    \n",
        "    # 가중치 초기화\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype(np.float32)\n",
        "    W_out = 0.01 * np.random.randn(V, H).astype(np.float32)\n",
        "\n",
        "    # 계층 생성\n",
        "    self.in_layers = []\n",
        "    for i in range(window_size * 2):\n",
        "      layer = Embedding(W_in)\n",
        "      self.in_layers.append(layer)\n",
        "    self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "    layers = self.in_layers + [self.ns_loss]\n",
        "\n",
        "    # 모든 가중치와 기울기를 배열에 모은다.\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "    self.word_vecs = W_in\n",
        "\n",
        "  def forward(self, contexts, target):\n",
        "    h = 0\n",
        "    for i, layer in enumerate(self.in_layers):\n",
        "      h += layer.forward(contexts[:, i])\n",
        "    h *= 1 / len(self.in_layers)\n",
        "    loss = self.ns_loss.forward(h, target)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.ns_loss.backward(dout)\n",
        "    dout *= 1 / len(self.in_layers)\n",
        "    for layer in self.in_layers:\n",
        "      layer.backward(dout)\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IParDzNv3nN_"
      },
      "source": [
        "### CBOW 모델 학습\n",
        "\n",
        "CBOW 모델을 활용하여 학습을 진행해본다.\n",
        "일단 PTB 데이터 셋으로 대량 데이터 학습을 진행하기 위해서,\n",
        "데이터를 불러오는 코드를 작성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IAOtBYL3nCR"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import urllib.request\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "dataset_dir = '/content'\n",
        "\n",
        "def _download(file_name):\n",
        "  file_path = dataset_dir + '/' + file_name\n",
        "  if os.path.exists(file_path):\n",
        "    return\n",
        "\n",
        "  print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "  try:\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "  except urllib.error.URLError:\n",
        "    import ssl\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "  print('Done')\n",
        "\n",
        "def load_vocab():\n",
        "  vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "  if os.path.exists(vocab_path):\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "      word_to_id, id_to_word = pickle.load(f)\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  data_type = 'train'\n",
        "  file_name = key_file[data_type]\n",
        "  file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "  _download(file_name)\n",
        "\n",
        "  words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "  for i, word in enumerate(words):\n",
        "    if word not in word_to_id:\n",
        "      tmp_id = len(word_to_id)\n",
        "      word_to_id[word] = tmp_id\n",
        "      id_to_word[tmp_id] = word\n",
        "\n",
        "  with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "  return word_to_id, id_to_word\n",
        "\n",
        "def load_data(data_type='train'):\n",
        "  if data_type == 'val': \n",
        "    data_type = 'valid'\n",
        "  save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "  word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "  if os.path.exists(save_path):\n",
        "    corpus = np.load(save_path)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "  file_name = key_file[data_type]\n",
        "  file_path = dataset_dir + '/' + file_name\n",
        "  _download(file_name)\n",
        "\n",
        "  words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "  corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "  np.save(save_path, corpus)\n",
        "  return corpus, word_to_id, id_to_word\n",
        "\n",
        "corpus, word_to_id, id_to_word = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0y7I3h0Xg6"
      },
      "source": [
        "학습에 필요한 Trainer() 클래스와, optimizer를 코딩한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X01IwAFq0d3a"
      },
      "source": [
        "import time\n",
        "\n",
        "# 중복된 가중치 제거\n",
        "def remove_duplicate(params, grads):\n",
        "  '''\n",
        "  매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
        "  그 가중치에 대응하는 기울기를 더한다.\n",
        "   '''\n",
        "  params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "  while True:\n",
        "    find_flg = False\n",
        "    L = len(params)\n",
        "\n",
        "    for i in range(0, L - 1):\n",
        "      for j in range(i + 1, L):\n",
        "        # 가중치 공유 시\n",
        "        if params[i] is params[j]:\n",
        "          grads[i] += grads[j]  # 경사를 더함\n",
        "          find_flg = True\n",
        "          params.pop(j)\n",
        "          grads.pop(j)\n",
        "        # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
        "        elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "          params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "          grads[i] += grads[j].T\n",
        "          find_flg = True\n",
        "          params.pop(j)\n",
        "          grads.pop(j)\n",
        "\n",
        "        if find_flg: break\n",
        "      if find_flg: break\n",
        "\n",
        "    if not find_flg: break\n",
        "\n",
        "  return params, grads\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "# trainer (fit, 배치 생성, plot 역할을 한다.)\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_list = []\n",
        "    self.eval_interval = None\n",
        "    self.current_epoch = 0\n",
        "\n",
        "  def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "    data_size = len(x)\n",
        "    max_iters = data_size // batch_size\n",
        "    self.eval_interval = eval_interval\n",
        "    model, optimizer = self.model, self.optimizer\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "      # 뒤섞기\n",
        "      idx = np.random.permutation(np.arange(data_size))\n",
        "      x = x[idx]\n",
        "      t = t[idx]\n",
        "\n",
        "      start_time = time.time()\n",
        "      avg_loss, elapsed_time = 0, 0\n",
        "      for iters in range(max_iters):\n",
        "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "        # 기울기 구해 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
        "        \"\"\"\n",
        "        if max_grad is not None:\n",
        "          clip_grads(grads, max_grad)\n",
        "        \"\"\"\n",
        "        optimizer.update(params, grads)\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "        # 평가\n",
        "        if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "          avg_loss = total_loss / loss_count\n",
        "          self.loss_list.append(float(avg_loss))\n",
        "          total_loss, loss_count = 0, 0\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print('| 에폭 %d | 시간 %d[s] | 손실 %.2f' % (self.current_epoch + 1, elapsed_time, avg_loss))\n",
        "      self.current_epoch += 1\n",
        "\n",
        "  def plot(self, ylim=None):\n",
        "    x = np.arange(len(self.loss_list))\n",
        "    if ylim is not None:\n",
        "      plt.ylim(*ylim)\n",
        "    plt.plot(x, self.loss_list, label='train')\n",
        "    plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# adam optimizer\n",
        "class Adam:\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.iter = 0\n",
        "    self.m = None\n",
        "    self.v = None\n",
        "        \n",
        "  def update(self, params, grads):\n",
        "    if self.m is None:\n",
        "      self.m, self.v = [], []\n",
        "      for param in params:\n",
        "        self.m.append(np.zeros_like(param))\n",
        "        self.v.append(np.zeros_like(param))\n",
        "        \n",
        "    self.iter += 1\n",
        "    lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "    for i in range(len(params)):\n",
        "      self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "      self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "            \n",
        "      params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co3JITJdznng"
      },
      "source": [
        "CBOW 모델 학습 코드를 작성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "HYCDBBOtznI0",
        "outputId": "bfdbc3ad-a98c-4820-e235-68aff495c157"
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "\n",
        "# 모델 생성\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# 학습 시작\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
        "word_vecs = model.word_vecs\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "  pickle.dump(params, f, -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 | 시간 447[s] | 손실 1.44\n",
            "| 에폭 2 | 시간 438[s] | 손실 1.24\n",
            "| 에폭 3 | 시간 447[s] | 손실 1.18\n",
            "| 에폭 4 | 시간 445[s] | 손실 1.12\n",
            "| 에폭 5 | 시간 441[s] | 손실 1.03\n",
            "| 에폭 6 | 시간 438[s] | 손실 1.01\n",
            "| 에폭 7 | 시간 439[s] | 손실 0.95\n",
            "| 에폭 8 | 시간 437[s] | 손실 0.92\n",
            "| 에폭 9 | 시간 433[s] | 손실 0.88\n",
            "| 에폭 10 | 시간 437[s] | 손실 0.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUZdbA8d8hpBAIEEJApIXee+ggiIXm2ntZO7rWdS0b11XX1XVZ3XfXri+6rquvoq6oqAFREEQElCIgJVQpobeElkCSed4/5s5kJpkkk2RuZjL3fD+ffJjbn7kkc+Y+5TxijEEppZRz1Ql3AZRSSoWXBgKllHI4DQRKKeVwGgiUUsrhNBAopZTD1Q13ASqradOmJi0tLdzFUEqpWmXZsmUHjDGpgbbVukCQlpbG0qVLw10MpZSqVURkW1nbtGpIKaUcTgOBUko5nAYCpZRyuFrXRqCUUlVRUFBAdnY2+fn54S6KrRISEmjVqhWxsbFBH6OBQCnlCNnZ2SQlJZGWloaIhLs4tjDGcPDgQbKzs2nXrl3Qx2nVkFLKEfLz80lJSYnaIAAgIqSkpFT6qUcDgVLKMaI5CHhU5T06MhCs2ZXL8u2Hw10MpZSKCI4MBBNfWMDFrywMdzGUUg6Sk5PDK6+8UunjJkyYQE5Ojg0lKubIQKCUUjWtrEBQWFhY7nEzZsygcePGdhULcHivoZ05ebRsXC/cxVBKOUBGRgabN2+mb9++xMbGkpCQQHJyMllZWWzYsIELL7yQHTt2kJ+fz7333sukSZOA4rQ6x44dY/z48YwYMYKFCxfSsmVLpk+fTr161f8Mc3Qg2KWBQClHeuLzNazddSSk5+x+ekMe/1WPMrdPnjyZ1atXs2LFCubNm8fEiRNZvXq1t5vnm2++SZMmTcjLy2PgwIFccsklpKSk+J1j48aNTJ06lddff53LL7+cadOmce2111a77I4OBEUuna9ZKRUegwYN8uvr/8ILL/DJJ58AsGPHDjZu3FgqELRr146+ffsCMGDAALZu3RqSsjguEGw/eML7+svVexjSPqWcvZVS0ai8b+41pX79+t7X8+bNY/bs2SxatIjExERGjx4dcCxAfHy893VMTAx5eXkhKYvjGotX78r1vn5r4dbwFUQp5ShJSUkcPXo04Lbc3FySk5NJTEwkKyuLxYsX12jZHPdEcOxk+S30Sillh5SUFIYPH07Pnj2pV68ezZs3924bN24cr732Gt26daNLly4MGTKkRssmxtSuevL09HRTnYlp0jIy/ZYzxnelf5tkBrVrUt2iKaUi2Lp16+jWrVu4i1EjAr1XEVlmjEkPtL/jnghKmjwzC4C5D4ymXdP6FeytlFLRx3FtBGU58+/zwl0EpZQKC9sCgYi8KSL7RGR1OfuMFpEVIrJGRL61qyy+BqYl18RllFIRqLZVhVdFVd6jnU8EbwHjytooIo2BV4DzjTE9gMtsLItXq+TEmriMUirCJCQkcPDgwagOBp75CBISEip1nG1tBMaY+SKSVs4uVwMfG2O2W/vvs6ssvgqKXCQnxnL4REGpbWkZmTx6XneuH9qW7YdO0D61QU0USSlVA1q1akV2djb79+8Pd1Fs5ZmhrDLC2VjcGYgVkXlAEvC8MebtQDuKyCRgEkCbNm2qddEilyE1KZ5xPU9j6o87Sm1/8ou1PPnFWgC+zxijKSiUihKxsbGVmrXLScLZWFwXGABMBMYCj4pI50A7GmOmGGPSjTHpqamp1bpoQZEhpk4d/npx7wr3PXz8VLWupZRStUE4nwiygYPGmOPAcRGZD/QBNth50SKXi9iY6J+lSCmlghXOJ4LpwAgRqSsiicBgYJ3dFz10/BSnCl1B7VtQFNx+SilVm9nZfXQqsAjoIiLZInKziNwuIrcDGGPWAV8Cq4AfgTeMMWV2NQ2Vldm5ZO1x5/uY/btR5e570SsL+Xh5tt1FUkqpsLKz19BVQezzLPCsXWWoSMdmDfjwtqFc/r+LytznqzV7ubh/5VrglVKqNnHkyOIh7YvzClU0wOzLNXvsLo5SSoWV4wJBUkJdurVo6F0W0YZjpZSzOS4QFLkMdetU7sP/uKauVkpFMccFgkKXoW6M/9v+9sHR5R5z3osLOHjspI2lUkqp8HFcIAj0RNA2pT5bJ09k+aPnBDzmlwPHGfDUbJ3URikVlRwVCIwxFLkMMWVUDTWpH1fu8T0fn8XSrYfsKJpSSoWNowJBkcuddbCybQS+Ln2t7K6mSilVGzkqEBRagSCmTtlve2SnphWeZ+GmA96gopRStZ0jA0F5TwR3jO5Y4XmufuMH/vH1+pCVSymlwslRgaCoyAoE5SSd69YiKahzLdt2OCRlUkqpcHNUICh0uZPIldVYDNA4MY6tkyfSsVn5k9Is3nKIjXuPhrR8SikVDg4LBJ6qodC87XP+OT8k51FKqXByaCCouNfQ+J6nAXBml+pNhKOUUpEunBPT1LhCa36B8toIPO47uzM3j2hH48Q40jIyy9wv71QR9eJiQlZGpZSqaY58IiivjcCjTh2hcWL5A8wAuj32JeOe0yoipVTt5axAYPUaio0J7dv2THSjlFK1kaMCgWfqycqOLH5oXJcK90nLyOSAJqZTStVCjgoE3hQTlZy8/o7RHfn8rhEV7rcqO6dK5VJKqXByVCDwjCOoSvfRXq0a0bxhfLn7TFu2s0rlUkqpcHJWICiqXtK5jPFdy92e+fNubThWStU6zgoE3qqhqr3thgmxAFzY9/Qy98nac5RDx09V6fxKKRUOjgoEnsbiYLqPBjKmazP+cXkf/nZp73L325WTV6XzK6VUODgqELhM8OMIAhERLu7fivi6Mbz+6/Qy9zvvxQXsyc3nSH4Bxmi6aqVUZHPUyGLPZ3LVp6UpVtFsZkP+OgdwtyvcPqpDCK6olFL2cNQTgScQ1JHqh4JgHyqmLcv2VkkppVQkclQg8FQNhSAOkJQQ3MPUxn3H6PTIzOpfUCmlbOKoQBDK2vqOzZJ47dr+PDi24lHHAH+duS6EV1dKqdBxViAIYdUQwLieLfhV77K7kvr632+3hOSaSikVag4LBKGrGvJITXKPNn7+yr6hO6lSStUgZwUC699QPREA1IuLYevkiVzQt2XIzqmUUjXJUYEglI3Fgbx362B7TqyUUjayLRCIyJsisk9EVlew30ARKRSRS+0qi0coxxEEMqxDU64f2rbM7dqNVCkViex8IngLGFfeDiISA/wN+MrGcnh5qobErkeCCs593wcrbLuuUkpVlW2BwBgzHzhUwW53A9OAfXaVw5cdjcXlaZ9a32/5i1W7a+bCSilVCWFrIxCRlsBFwKs1dc1Qdx8NxPfUgSazSX9qtk95DJNnZrF6Z65t5VFKqYqEs7H4OeD3xpgKK85FZJKILBWRpfv376/yBb2NxVU+Q8XE5+yBktsdOHaS69/8kUPHT3HsZCGvfbuZS15daGOJlFKqfOFMOpcOvG/VqTcFJohIoTHm05I7GmOmAFMA0tPTqzxA2NtYbGMk8D13bBnzHny7YT/9n/zau3yyUBuRlVLhE7YnAmNMO2NMmjEmDfgIuCNQEAjpNa1/7awauiy9FQDzHzyTOgIX9dPxBUqpyGZn99GpwCKgi4hki8jNInK7iNxu1zUr4qqBuQG6ntaQrZMn0iYlERHhn1foiGOlVGSzrWrIGHNVJfa9wa5y+F/I/U9N9Rqqihk/72Z4x6Y0qhcb7qIopRzCURPTeJ4I7Kwaqqp1u4/w/o/b+c+ibQzvmMK7twwJd5GUUg7hqEBQPKAsrMUIaPzz33lff7/pIMYYWwe+KaWUh6NyDdXEOIJQcfk0Z7hcRuc+VkrZxlGBoCbGEQRyTvfmlT4m58Qp7+v2f5jBpa8tCmWRlFLKy1GBwPuduoYjwY3D0yp9zICnZvPat5u9y8u2HQ5hiZRSqpijAgFhaiwe1qEpWydPZMp1Ayp13OSZWfxy4LhNpVJKKTdHBQKXzWmoKzKkQ0qljznz7/P8lrP2HOHAsZMhKpFSSjksEJgwdx9tmBBLp2YNqnz8quwcxj33HelPzWbWmj0hLJlSyskcFQhcETCgrENq1QPB+S9973192zvL+EqDgVIqBBwVCLzjCMJWOQR/v7wP/7lpELN+e0a1zzXpnWUcPl7cu+iDJdt5/8ftpGVkkptXUO3zK6WcwVkDyjzdR8MY/hrE12VU51QAGibU5Uh+YbXOt3n/MdLrN2Hj3qP8ftrP3vVb9h+jX5vkap1bKeUMznoiCHNjcUn/d0v1J7v/cvUeck8UlEplvSsnnwUbD1T7/Eqp6OesJwIiK9dQ3TruONz1tCSy9hyt0jneWPALbyz4hS7Nk/zW3/necgC2Tp5YvUIqpaKeo54IIqGx2FdcXXdB6sXFVPtc6/dWLZAopZSjAkFx1VBkRIIOqQ14aFwXXrmmf7iLopRyMGcFAqtqKFKeCESEO0Z3pEWjekz7zTBevjr0AWFu1r6Qn1MpFV2cFQgirGrI14C2yUzs3SLk5/1oWXbIz6mUii4OCwSe7KMRGAlsskHbDpRSFXBYIHD/WyeC48BpDRP8li/u37Ja59u471i1jldKRT9HdR8t7jUUuZEg854R7DmST35BEQmxMby5YGu4i6SUinKOCgTF4wjCXJBypDSIJ6VBvHd5eMcUpi3Xen6llH0cVTVUG54ISrq4f6twF0EpFeUc9URAlMz72yC+LsdOBp+jaMr8zSzcfJAt+4+TXD+O6XcOx+Uy5BcWkRjnrF8BpVRpjnoiMER2tVBFPHMfz3twNOBuWL73rE4VHvf0jCzmrd/P9kMnWLkjB4C/f7We7o/NqlRAUUpFJ0d9HXQZU6uqhTwemdCNxPgYrkhvTUGRoV5cDIsfPoumDeLYnZvP83M2Vvqcn/y0E4AjeQU0iHfUr4FSqgRHfQIYUzufCG49o733dV0rLdFpjdzdTKsS1/702Rp25+YHtW9BkYtThS7qa7BQKmo5qmrIZaJ3MFnLxvWC3vethVu9rw3w2PTVpGVkcue7y0vte/s7y+jx+CzSMjJDUEqlVCRyVCAwmMiZjCBE4q1HhLYpid51Q9unBH38RS9/z9uLtgGQ+fPuUtvnaK4ipaKes573a2nVUHlSk+J549fpDExrwuYDx8g7VUTvVo3YvP84d767nJ05eeUev+/oyTK3rbAalsvimQ6zUb3YyhdcKRUxHBUIXMZEZdXQ2VZvov4+U1P2bd2YXw9ty19nZlX6fGkZmTRtEM+BY/5BInPVbkZ2bsr8DfvpkNqA8c9/B+jkN0rVdo4KBLW1sbgmLdt2CKBUEAD3rGf92zRm+fbynxSUUrWLo9oIXKZ2jSoOh0teXVTu9rKCwN4j+Rw8dpKfs3NJy8j0jldQSkU+254IRORN4DxgnzGmZ4Dt1wC/x918exT4jTFmpV3lAXdjsZPCQKDGXzvc/+HKUvmQ5mTto0/rxjVyfaVU9QT1RCAi94pIQ3H7l4gsF5FzKzjsLWBcOdt/AUYZY3oBTwJTgipxNRgTmZPS2CVrT83MRRAwKV6UpPNQygmCrRq6yRhzBDgXSAauAyaXd4AxZj5wqJztC40xh63FxYDt2dVMLR1ZrJRSdgo2EHg+PScA7xhj1hDaHvk3AzPLvLjIJBFZKiJL9+/fX+WLGJz1RGDC+K38vzpFplK1RrCBYJmIfIU7EMwSkSTAFYoCiMiZuAPB78vaxxgzxRiTboxJT01NrfK13L2GnBMJwplZdHduPsc1oZ1StUKwgeBmIAMYaIw5AcQCN1b34iLSG3gDuMAYc7C656uIexyBczw0rktYr6+BQKnaIdhAMBRYb4zJEZFrgT8CudW5sIi0AT4GrjPGbKjOuYLlrhpyTihIjIsJ6/Ufnb46rNdXSgUn2EDwKnBCRPoA9wObgbfLO0BEpgKLgC4iki0iN4vI7SJyu7XLY0AK8IqIrBCRpVV7C8FzNxbbfZXI4Qqy8i41Kb7inapg1pq9tpxXKRVawVYiFxpjjIhcALxkjPmXiNxc3gHGmKsq2H4LcEuQ1w8JE30558rVuklihfv85aKeXDO4LTe/tUQTzCnlUME+ERwVkYdxdxvNFJE6uNsJahWnNRYPateEL+4eAcBZXZsF3OeawW0BeOnq/nx+14gaK1t15OYV4HLpOAWlQiXYQHAFcBL3eII9uPv8P2tbqWzicljVEEDPlo2Y/+CZvHxNf++6924ZzJz7R7HxL+O96+rFxdCrVSPO693Cu25irxbUhBk/72bBxgNB7Xvg2En6PPEVL3xT+VnZlFKBBRUIrA//d4FGInIekG+MKbeNIBK55yx2WCQA2qQkkhAbw3cPncl7twxmWMemdEhtQGxM6f/+l67uz7OX9qZ9an0GpiUHOFvo3fHucq791w9B7bvviDsZ3per99hZJKUcJdgUE5cDPwKXAZcDP4jIpXYWzA4uh6c9aN0kkWEdm1a432Xprfnm/tFcMbANY7o246Wr+wHQrAqNyvkFRd7Xx08Wkl9QxM6cPHJOnCq177aDx/nv0h3MW+/fVvHsrCzSMjJZt/uI94nO4f+VSoVUsI3Fj+AeQ7APQERSgdnAR3YVzBYOyzVUXfXiYnjzhoEYY1g67DBXDGzNtoMnuP3/lpXa94ZhaX5TYHpc8upCMu8ZCUCPx2fRpkki2w+dAODc7s35am1xz6JRz87zvp712zPokFqfRVsO8vLczQCMf/47Zt470u/8Ow6dQARS6sfT7bEv+ftlfbh0gO3ZSpSKKsEGgjqeIGA5SC1MYe3UqqHqEhH+dH4PALq1aBhwnz+d34PHf9Wd8c9/55fsbs2uI37zHXuCAOAXBEr65cAxvli1ixe/2eS3/uJXFnpfFxa5GPnMXABeu3YAAM/P2aCBQKlKCvbD/EsRmSUiN4jIDUAmMMO+YtnDiY3FdrioX0sAfnt2J7/1IlJmoKgsYygVBADyrKomg6HQp+fQxr1HvccppSonqCcCY8yDInIJMNxaNcUY84l9xbKH08YR2KV1cr0yt4Xq/j740apyt2/Ye4wj+QWl1mcfziP3RAGNEmtd72alwiborGTGmGnANBvLYjuXMVo1FAKeL92C8MMfzvJOYm+tDIljQeQpGvSXOd7XP24tznj+zfq9XNRPq4eUCla5VUMiclREjgT4OSoiR2qqkKFiQB8JQuC6IW3p06oRVw1qTfOGCXRunuTdduvI9mEp03c+4xDu+8DWie6UijrlBgJjTJIxpmGAnyRjTGgqg2uSw0YW26VZwwSm3zWCZg0TSm3r1qIhWydPDEOplFJVVet6/lSH09JQK6VUMBwVCJw2Z7Gq2J7cfL9Bb0o5kbMCAdpYrPwN+escbn3b9gzoSkU0RwUCTVipPNbvOcr5Ly0A/BuaK8vlMmGdG1qpUHBUIHBaGupw6pBaP9xFKNczX2axKrv8SfaG/nUOZ//j2zK3//v7X2j/hxlc9tqiUBdPqRrlsECgI4trypz7R/O3S3rRu1WjsJajqIxv7HuP5vstf/JTNgA7c/JwuQwvz93E7tx8Nu075p17+VShy3uur9bs4YnP1wKwdNthO9+CUrYLekBZNHDPWRzuUjjHFQPbcHl6a/614BeeylwHwKd3Dmf/0ZM1Ui9/4lQh3R+bBcDnd42glxWUXpm3idU7/YfBTP1xBx1SG3D+S9+XOk/vJ77iwbFdmDwzi7vO7MgDY7sw6Z3SifeUqq2cFQh0ZHGNExFuGdme+vF16Xl6I++Hsa/hHVP4ftPBkF73tneW+s2Z/KuXFvDtg6Npm1KfZ75cX2r/tbuOsGX/8YDnKnIZJs/MAuCluZsY1jGl1D4fLcvWZHeq1nJU1ZBLcw2FzVWD2gQMAn+5qCfPXdGvVJvCb8/uxPASH7jPX9m31PHT7xxeah3gFwQ8ck6Uzk3kEUxKC4+rXy89ic4D/11ZqXMoFUkcFQjcVUMaCiLBwowxrP3zWK4Z3JbUpHjm3D+apAT3A+qaJ8Zyz5hOPHdFP79jhgeYVMcAky/uFdQ1/zl7A0t8chKV9NsPVgT/BgLo+fgsVu7IqdY5lAoHx1UNaRyIDKc3Lp3BdMFDY8grKKJ+vPvXMjUpng1PjWf/sZPExghNG8Tz4NgunN2tOWOfmw+4/0+vHNSGjI9/BuCqQa2Z+uOOgNect34/89bvt+kduS3ddpg+rRvbeg2lQs1hgUCrhiJZo8RYGuGfPjqubh1a+gSNO8/s6Le9ZH+gTfuO2VU8paKWw6qGtLE4WpzdrRkAqQ3851EOd9WfDi5TtZGjAoHLpd1Ho8WU69JZmDGG1k0SAcgY35W+rRuH/YP4qcx17PCZjrM8n/60k4+Xu8cvnCp0BZ3zaPXO3LC/TxVdHBUIDAbRyqGoUKeO+LUz3D6qA5/eOZz4ujFhLJXbF6t2B7Xfbz9Ywe8+dM+dMPa5+XR99Msy980vKOJUoYuFmw5w3osL+Pf3W0NRVKUAh7URuDT7aNTbHuS3cTuJwI5DJ8jNK6BnS/8us7ty8oipIzQvMZfDLwcCj2Hw6Prol7RsXI+7x7jbSLL21Lp5oVQEc9QTARoIol5EBAJg5DNzOe/FBWw/WFyeIpdh2ORvGPz0nLIPLmFXTp63ymhnTl6oi6oU4LBAoI3F0S8pPvwPub6/Y2c8O9f7QV7ocnnX+9bxLwuQq2jHoRMs3XqIYZO/4cZ/Lym13beK8/8Wb2P1zvIT6ClVnvD/1dQgrRqKfnF168DJ8Jah5O/Yy3M3cf+5XfCJA7R7eIb39SWvLvTbPy0j02950ZbA6TdOnCrkZIGLP366GkCnCFVV5qhAYIw2Fkc7VwT2pnnxGysQBFG2kkGgJM/AuQ+W7uCDpYEHzilVWQ6rGtIngmiXnBhX7vaRnYrTVHx+1wi2Tp7I1FuH+O3z4W1D/ZYHt2tS7jmbN/Qfy7Bh79FS+6RlZPLE52vKPY9S4WJbIBCRN0Vkn4isLmO7iMgLIrJJRFaJSH+7yuLhrhrSSBDNXr7G/9fovVsH08vquTOux2m8cX26d5snCd7QDilseXqCd/3AtGTaNS1OgvfBbUM5o3NqmdesI8LbNw3yLn+4NDvgfmWtjxS5eQVc968f2JObX/HOKqrY+UTwFjCunO3jgU7WzyTgVRvL4mYMdTQORLVuLRr6ZSkd1qEpn9/t/ub/2nUDiK8bw5MX9uSm4e38jqtTR2hoJb0TEX4/rgsATa2Ry2/fNIixPZoD7oDid6yIX+CorT79aSffbTzAK/M2hbsoqobZ1kZgjJkvImnl7HIB8LZxd59YLCKNRaSFMSa40ThVoGmoneH8PqezMyeP83qdHnD7dUPaBlw//a4R/GA1zI7r2YLVT4ylrs83h2cu6cOozru5alBrb2NvvzaNefDcLtSJ8G8YRS7DjW8t4c7RHRjcPoUffzlE71aNSIgtHoDneQuetowdh06w90g+6WnlV42p2i+cjcUtAd/WrmxrXalAICKTcD810KZNmypf0GC0asgBRIQ7RneseMcS2jWt7/fNvkGJrqiNEmO5erD7929gWjIJsTG8c/NgAHbnRnYf/wPHTjJ/w36ydh/hkYnduPf9FVyR3pq/Xdq7eCfrb8NltWmPfGYuoL2RnKBW9BoyxkwBpgCkp6dXuVuIe/L6kBVLOdh/bx/mtxwT4V8wPKVzGbj3ffe8C+tKjE72/G2U7Ny0YOMBth06jjFw7ZC2rNt9hFvfXsrnd40guX75jfOqdghnINgJtPZZbmWts437m05k/8Gq2ilSnzRz8wqY+uN2Lu7X0lpT/Clf8gPf07X6i5W7+M2oDt711/6reEa23LwC5mbtI/twHoOfnsP6p8ZF7HtXwQtn99HPgF9bvYeGALl2tg+AZ85iO6+gnComgn6xDh476R3N/JfMtUyemcUgK62F74f/zztz2XbQneMo58Qp/vCJe4zC0ZOFnPHs3IDnfnbWepZaI6FPFbn4aFlk94RSwbGz++hUYBHQRUSyReRmEbldRG63dpkBbAE2Aa8Dd9hVFg+jI4uVTSIhDjw/eyP5BUUMeGo2N721hAPHTrJml3/1T1GJx4BRz87DGMPkmVlVuua8DfbO+FbSrpw8TadhAzt7DV1VwXYD3GnX9QNeU9NQK5sUFIV/RPM/Z29gwz73YLaFmw+S/tTsUvvknCgotc433UVlZa7azctXV/nwShs2+RtAG7BDzVkjiw3UcdQ7VjUlOTG24p1K2Dp5ot9AtkBuG9W+wvNcObC4qe27Gv6GDu52A1W7Oepj0aW5hpRN6sbUYWLvFqXWNymjV83/XNYHoMLxB62SE/2WL+7f0vs6Y3xXHv9Vd7/G2lNFLmpaYYivmXPiVIWztW07eJy8U8HN6KYqViu6j4aKdhpSdnrpqn5k+sxONveB0bRsXM+dERX/hHKXDGhV6vjTGiaw50g+1w1pyzuLtwEQWyJQ9GnVmKsHteGn7Tnceob7aSFj2irv9vyCmg8Eh46fIqXE3NGB7MnNp3nD+DJ7GX2/6QCnilzc+O8ldD0tib9f1odCl+F/v93MOd2bc3H/4ns26tl5DOuQwnsl8kSpqnFUIMCg8xEo25T8gCsr7cTcB0YHXD/vwdEczS8kNSmeRyZ2Y3duPj9tL56r4OWr+zOh12mIiN9o3/lhqA7y9cWq3dx3TlK5+6zZlcvEFxZwz1md6Ny8Aef1Pp19R/J5fs5GHpnYjcS4ulzzRnE31aw9RznvxQXe5Zmr93gHunks3Bw4PbeqPAdWDSlln7vOrHhEc8kAcWaXVOLq1iEhNobUJPc364TYGNo1re/X3XNUl9SA36Z3hTlJXFnN5Gt3HfF2L916wD1T2wtzNnLXez9xJL+AQU/P4d0ftvPY9OCysj7w35WhKK4KwFFPBIbI6OanotcdZ3bgpbmbaN2kXqltr107gJaNS6//942DSq3z8HzINm0QXyrlRcQoY56FCS98B8DF/VrywpyNftsKfXpZrd11pMI2gXBzuQw7c/Jo3SSx4p1rIec9EWjVkLJRYlxdnrmkN+9PGlpq27iep3lTXwdrbI/mDG7XhI9/M6zincNk+fYc0jIyWZWdE3D7f5ftYH2JORpe/KY4MKzdfYSuj35paxmrYt+RfN3LPkoAABLVSURBVHZZ80S/+M0mRj4zl18OHA9zqezhqEBgNPuoqgGXD2wd8Jt/VSQlxPLBbUNpkxK530S37D8GwPkvfc+ybYdIy8jk3vd/8m7//bSfSx3z7++31lTxqmzQ03O84xYWW1lpPYEh2jgvEOgTgVIh5dtGccmriwCYvmJXuIpTLUfyC/jxl0Ol1ntSiBSVbLGOEg4LBEZTTCjlo2OzBn7LZbVDvP7rdLZOnug3ondA22Rby1ZVuXkF7D1SHJymr9hJWkYm2w+eqPDY299ZxuX/u6jUIDljtdbMWrOHfUeibwY3ZwUCtLFYRZ8/TOjqtxwXU4ebhrcjNqb4l/3BsV38tvsem3nPCO9y+9TiHk2+czWf07259/UV6a156ep+fueMJH2e+IrBVpK9z1bu8qbd/tusLEwZDdsea3e7czMt2HjAu2759sN8v8ldNfTuD9sZ9PQcThXW/HgNO0VoNwR76MhiFY1iSuRNmXRGex4Y24X9x07y+Up3FY3vN/8nL+zB76f9zHm9WzCmq/sD/r1bBnPvByv4z42DWJGdw5ldmgHwz683sGSrf1WJZzIb3w/LSLFp3zG/5Xnr93lfZ67azbndm/P2om0s23aYDyYNYXD7FO92Y4w3F9Od7y33rr9yyuJS1yl0uYirxvdoYwy7c/M5PURtSdXlqECg2UdVNLthWBr92yaXmlM5pX4cDROKcyGN6JQKwBU+OYqGdWzKkkfOBvAGAYD7zulc5vXqxkTOH9PJwiJ+98FKMn/2z2S/Yrt/T6Y56/axzEqj/fAnPzOkfQp/nNiNF7/ZxDuLtgU8tx3f/t9auJUnPl/LzHtH0q1Fwwr3zztVRG5eAac1Sgh5WcBpgQBtLFbR7fw+xfM0923dmM9X7uKN69PpelpDurdoyNMX96Jl43ohyd4ZSX9JX63ZWyoI5BcUsaVEd8/PVhY3Ym/Zf5wt+4/z3g/bK3296rYZe3ohbTt4PKhAcPUbi/lpe45tWVedFQi0sVhFobJ+pW8ansaozqneaqEZ944M7XUj4I/pZGERhUWGu6f+VGqbnWMTQtV76E+frWVcz9LJCkv6aXvgMRqh4qzGYp2zWEWhcT1Po15sDFcPbuO3XkRK9QoKpQiIA3T545f0eHxWjV/XFaJAsOdIfsCg4nIZNpYYhAdU2NhdVY4KBNpYrKLR6Y3rse7JcXRuXn7it1AL9V9Ss6SKM5hGClcZH8j5BUXcPfUndloDz56bvYEPl+wo91yvf7eF52ZvIC0jk8emrwbg5v8s4Zx/zvdWIXlsDaILbFU4KhC42wjCXQqlokN1/5b+dkkvv+Xrh6X5Lf/5gh58dd8ZpY779sHRdEgNnNm1phQZw6Z9x/gma6/f+rlZ+/h85S6e/HwtAM/N3shD01Yx5n/mkXPiFD0fn8XvP1rFNp8P9Mkzs3hutjvlxtuLtvHGd1uYu96dUfbKKYv9em0dzbdnEiBnBQJNQ61UCFXub6l5w3hm/674g/2KgcVVWcsfPYfkRP9JfK4b0pbOzZN452b/pHxtU+p7R/qGizFw9j++5aa3lgbc/uWaPX7zT2zZf5y+f/6aYycL+WDpDrL2lK728Xgqc53f8h8+Lk7R8eQXa6tZ8sAc1Vhc1uOcUqryfL9TfXH3CL/5A7ZOnuj9ILxnTEeWbT/Mu7cMKVUfnnnPCIxxz+Q2yBrA1qtlI0SKG6NHdkpl01/Gk/nzbnq2dCftC3cVb8n3sSsnj4kvfOfXaytUNvqMjTh+0p4srY4KBOg4AqVCxvOnFFe3DikN3N/mh7ZP4d1bBvvt97tzi0cgl/wm3+P04mysHZs1KLN7ZN2YOlzQt3iazuzD9tSVB8v3S+XSrYe49DV3jqX/lDEWIdI5KhC4U0xoJFAqFDyT6Fw/tC0tGtXj0zuH0/W0JO88zP9382DbBp0dD/N8xS6fMWaeIFAj17WpVsNRgUBnKFMqdFolJ/LN/aNoY03W0rd1Y7/tIzo1DXjchF6nMePnPbaXz06bDxyreCcb2JXjyHmNxTqQQKmQaZ/agLoxlfsYeeWaAbaNkK2uV6/pX2rd+X1O5/kr+/qtu/HfS2qqSH5KjpQOFX0iUEop4IFzOzO+V/Eo30fP644xhqsGtaF+fF1vFtNo5KwnAoisBClKqZC7ocR4hEAu7teSsT2a+zVs3zWmk9/xN49oxy0j21M/UueKDqHof4e+dByBUlGpfWp9+rdJ5pEJ3UiuH8dbC7cCsPEv4+n0yEwA3rwhncIiw7klsrMCJCUUfxT+6fwe/On8HmVeZ8v+8M1bfMuIdrac11GBQKuGlIo+t53RnrvGdCTJJ9V20wZxnDhVRGxMHRonxpJzooB+rZNJrh9X6vgPbxvqbfAuz9bJE5m2LJv7/7sypOWvjIfGda14pypwVCDQ7qNKRZflj55DkwAf7osfPsv7+tVrBvDGd1toVC+21H6AdyBbMPYeDe80lXF17anNd1QgcGkaaqWiwn1nd+bEqcKAQQDw68k0tEMKQzukBNyvsmJs/gBZ++exdH+s5rOpOioQGKNtxUpFg3vP7hSW69qd4yi+boyt5y+Lrb2GRGSciKwXkU0ikhFgexsRmSsiP4nIKhGZYFdZPHm8I2EyDaVU7VTVQLB18kQeHl+6fr9pg+InmjVPjA04X0pNDH2y7YlARGKAl4FzgGxgiYh8ZozxTZ/3R+BDY8yrItIdmAGk2VEez8hsjQNKqVB4/sq+fmMLZtwzkgc/Wskdozvy/pLt3DAsDZfPZFi3jepAvbgYmjdMIC6mDi/P3cSHtw2l/R9mAJTqptrj9Ias2XWEJy/syYdLs+lXYuR2KNlZNTQI2GSM2QIgIu8DFwC+gcAAngk7GwG7sIknQ0e4sxYqpWqvhFh31U3LxvW4oG9Lv0DQ/fSGZN7jng50Yu/A00/+emia9/WZXZv5rG/rfT2h12lcNqA1X6zazZpdR4iNqcP0O4eH8m2UYmcgaAn4Ts2TDQwusc+fgK9E5G6gPnB2oBOJyCRgEkCbNm0C7VIhT7ImzTChlKquER398yg9MqFblc/1y1/9a8RfuWYAAI0TY5m2PLvUtewQ7sbiq4C3jDH/IyJDgXdEpKcxxi+zkjFmCjAFID09vUrp97RqSClVXYVF7o8mT1bVuQ+MRoC0plWfMa2sdst+bZJrLCeTnYFgJ9DaZ7mVtc7XzcA4AGPMIhFJAJoC+0JdGIM2Fiulqic1KQGAdtYHf7tqBIBIYmevoSVAJxFpJyJxwJXAZyX22Q6cBSAi3YAEYL8dhdEnAqVUdY3t0Zz/3DSIm4bbk+ohXGwLBMaYQuAuYBawDnfvoDUi8mcROd/a7X7gVhFZCUwFbjDGnpkXvIFAG4uVUlUkIozqnBp16extbSMwxszA3SXUd91jPq/XAvY2h3uuhTYWK6VUII5JQ+3SqiGllArIMYHAeLuPaiRQSilfjgkELnvmfFZKqVrPMYEAb9WQPhEopZQvxwQC7ziCMJdDKaUijWMCQZFVN2R3GlmllKptnBMIPI3FGgiUUsqPYwKBy8peVFcDgVJK+XFMICi0IoHdU80ppVRt45hA4Hki0KohpZTy55hA4GkjiHHMO1ZKqeA45mPR02tIRxYrpZQ/xwQCl9Huo0opFYhjAoF3HIE+ESillB/nBQJ9IlBKKT+OCQQ/bT8MQHxsTJhLopRSkcUxgWBkp1TGdG3G4HZNwl0UpZSKKLbOUBZJ0prW580bBoa7GEopFXEc80SglFIqMA0ESinlcBoIlFLK4TQQKKWUw2kgUEoph9NAoJRSDqeBQCmlHE4DgVJKOZwYKytnbSEi+4FtVTy8KXAghMWprfQ+6D3w0PvgnHvQ1hiTGmhDrQsE1SEiS40x6eEuR7jpfdB74KH3Qe8BaNWQUko5ngYCpZRyOKcFginhLkCE0Pug98BD74PeA2e1ESillCrNaU8ESimlStBAoJRSDueYQCAi40RkvYhsEpGMcJcnlETkTRHZJyKrfdY1EZGvRWSj9W+ytV5E5AXrPqwSkf4+x1xv7b9RRK4Px3upDhFpLSJzRWStiKwRkXut9Y65FyKSICI/ishK6x48Ya1vJyI/WO/1AxGJs9bHW8ubrO1pPud62Fq/XkTGhucdVZ2IxIjITyLyhbXsuHsQNGNM1P8AMcBmoD0QB6wEuoe7XCF8f2cA/YHVPuueATKs1xnA36zXE4CZgABDgB+s9U2ALda/ydbr5HC/t0rehxZAf+t1ErAB6O6ke2G9lwbW61jgB+u9fQhcaa1/DfiN9foO4DXr9ZXAB9br7tbfSTzQzvr7iQn3+6vkvfgd8B7whbXsuHsQ7I9TnggGAZuMMVuMMaeA94ELwlymkDHGzAcOlVh9AfAf6/V/gAt91r9t3BYDjUWkBTAW+NoYc8gYcxj4Ghhnf+lDxxiz2xiz3Hp9FFgHtMRB98J6L8esxVjrxwBjgI+s9SXvgefefAScJSJirX/fGHPSGPMLsAn331GtICKtgInAG9ay4LB7UBlOCQQtgR0+y9nWumjW3Biz23q9B2huvS7rXkTVPbIe7/vh/kbsqHthVYmsAPbhDmKbgRxjTKG1i+/78b5Xa3sukEItvwfAc8BDgMtaTsF59yBoTgkEjmbcz7mO6ScsIg2AacBvjTFHfLc54V4YY4qMMX2BVri/wXYNc5FqlIicB+wzxiwLd1lqC6cEgp1Aa5/lVta6aLbXqubA+neftb6sexEV90hEYnEHgXeNMR9bqx15L4wxOcBcYCjuaq+61ibf9+N9r9b2RsBBavc9GA6cLyJbcVcDjwGex1n3oFKcEgiWAJ2sXgNxuBuEPgtzmez2GeDp7XI9MN1n/a+tHjNDgFyr2mQWcK6IJFu9as611tUaVr3uv4B1xph/+GxyzL0QkVQRaWy9rgecg7utZC5wqbVbyXvguTeXAt9YT02fAVdaPWraAZ2AH2vmXVSPMeZhY0wrY0wa7r/1b4wx1+Cge1Bp4W6trqkf3D1ENuCuL30k3OUJ8XubCuwGCnDXY96Mu45zDrARmA00sfYV4GXrPvwMpPuc5ybcDWKbgBvD/b6qcB9G4K72WQWssH4mOOleAL2Bn6x7sBp4zFrfHveH2Cbgv0C8tT7BWt5kbW/vc65HrHuzHhgf7vdWxfsxmuJeQ468B8H8aIoJpZRyOKdUDSmllCqDBgKllHI4DQRKKeVwGgiUUsrhNBAopZTDaSBQtZKILLT+TRORq0N87j8EupZdRORCEXmsgn2eFZEsK0vqJ56xAta2UhkyRSROROb7DKBSqkwaCFStZIwZZr1MAyoVCIL4cPQLBD7XsstDwCsV7PM10NMY0xv3eJiHAUSkO+5BUz1wJ8Z7RURijDu54hzgCttKraKGBgJVK4mIJ8PmZGCkiKwQkfushGvPisgS69vzbdb+o0XkOxH5DFhrrftURJZZefsnWesmA/Ws873rey1rBPKzIrJaRH4WkSt8zj1PRD6yvrW/a41yRkQmi3t+hFUi8vcA76MzcNIYc8Bani4iv7Ze3+YpgzHmK1OcMG0x7nQHUH6GzE+Ba0Jwu1WU08dGVdtlAA8YY84DsD7Qc40xA0UkHvheRL6y9u2P+1v1L9byTcaYQ1YqhiUiMs0YkyEidxl30raSLgb6An2AptYx861t/XB/K98FfA8MF5F1wEVAV2OM8a3O8TEcWO6zPMkq8y/A/bjnEijpJuAD63VL3IHBwzdD5mpgYIDjlfKjTwQq2pyLO3/QCtwpqFNw54gB+NEnCADcIyIrcX+QtvbZrywjgKnGnd1zL/AtxR+0Pxpjso0xLtypLdJwpzPOB/4lIhcDJwKcswWw37Ngnfcx3Hlx7jfG+M0zISKPAIXAuxWUFWNMEXBKRJIq2lc5mz4RqGgjwN3GGL8kcSIyGjheYvlsYKgx5oSIzMOdc6aqTvq8LgLqGmMKRWQQcBbuZGZ34c6E6SsPd7ZLX71wZ788vcR7uAE4DzjLFOeGqShDZjzuYKRUmfSJQNV2R3FPS+kxC/iNlY4aEeksIvUDHNcIOGwFga74V8EUeI4v4TvgCqsdIhX3FKFlZqMU97wIjYwxM4D7cFcplbQO6OhzzCBgPO6qpgesrJeIyDjcjcrnG2N8nyzKzJApIinAAWNMQVllVAr0iUDVfquAIquK5y3ceefTgOVWg+1+iqck9PUlcLtVj78e/3r2KcAqEVlu3OmLPT7Bndt/Je4spw8ZY/ZYgSSQJGC6iCTgflL5XYB95gP/Y5U1Dngdd7bTXSJyP/CmiIwBXsL97f5rqx16sTHmdmPMGhH5EHcDeCFwp1UlBHAmkFlG2ZTy0uyjSoWZiDwPfG6MmR3i834MZBhjNoTyvCr6aNWQUuH3NJAYyhOKewKmTzUIqGDoE4FSSjmcPhEopZTDaSBQSimH00CglFIOp4FAKaUcTgOBUko53P8D1fWQSN6x0N4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-GD8dNZzzyB"
      },
      "source": [
        "위에서 학습한 단어의 분산표현 성능을 평가해보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ATUY1ZLzt37",
        "outputId": "3053758a-be29-4f0a-86db-324d62cf645f"
      },
      "source": [
        "# 코사인 유사도\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "  nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
        "  ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
        "  return np.dot(nx, ny)\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_vecs, top=5):\n",
        "  if query not in word_to_id:\n",
        "    print(\"%s을 찾을 수 없습니다.\" % query)\n",
        "    return\n",
        "\n",
        "  print(\"\\n[query] \" + query)\n",
        "  query_id = word_to_id[query]\n",
        "  query_vec = word_vecs[query_id]\n",
        "\n",
        "  # 코사인 유사도 계산\n",
        "  vocab_size = len(word_to_id)\n",
        "\n",
        "  similarity = np.zeros(vocab_size)\n",
        "  for i in range(vocab_size):\n",
        "    similarity[i] = cos_similarity(query_vec, word_vecs[i])\n",
        "\n",
        "  # 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "  count = 0\n",
        "  for i in (-1 * similarity).argsort():\n",
        "    if id_to_word[i] == query:\n",
        "      continue\n",
        "    print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "    count += 1\n",
        "    if count >= top:\n",
        "      return\n",
        "\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "  params = pickle.load(f)\n",
        "\n",
        "  word_vecs = params['word_vecs']\n",
        "  word_to_id = params['word_to_id']\n",
        "  id_to_word = params['id_to_word']\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "  most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " i: 0.7119140625\n",
            " we: 0.7099609375\n",
            " they: 0.61962890625\n",
            " your: 0.603515625\n",
            " anything: 0.57421875\n",
            "\n",
            "[query] year\n",
            " month: 0.875\n",
            " week: 0.78955078125\n",
            " spring: 0.76708984375\n",
            " summer: 0.7607421875\n",
            " decade: 0.68798828125\n",
            "\n",
            "[query] car\n",
            " luxury: 0.6337890625\n",
            " auto: 0.609375\n",
            " cars: 0.58740234375\n",
            " window: 0.57177734375\n",
            " merkur: 0.5712890625\n",
            "\n",
            "[query] toyota\n",
            " engines: 0.6484375\n",
            " mazda: 0.607421875\n",
            " weyerhaeuser: 0.6015625\n",
            " compact: 0.60009765625\n",
            " marathon: 0.59619140625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvZs5aNe4zsR"
      },
      "source": [
        "### 유추 문제 풀기\n",
        "\n",
        "word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 가까이 모을 뿐 아니라, 더 복잡한 패턴을 파악하는 것으로 알려져 있다. 대표적인 예가 'king - man + woman = queen' 같은 문제이다. analogy 함수를 활용하여 다음과 같은 문제를 해결해보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fC0yyru4c1t",
        "outputId": "9f3f22f9-33d8-42ae-d809-3b16aaf22391"
      },
      "source": [
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "  for word in (a, b, c):\n",
        "    if word not in word_to_id:\n",
        "      print('%s(을)를 찾을 수 없습니다.' % word)\n",
        "      return\n",
        "\n",
        "  print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "  a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "  query_vec = b_vec - a_vec + c_vec\n",
        "  query_vec = normalize(query_vec)\n",
        "\n",
        "  similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "  if answer is not None:\n",
        "    print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "  count = 0\n",
        "  for i in (-1 * similarity).argsort():\n",
        "    if np.isnan(similarity[i]):\n",
        "      continue\n",
        "    if id_to_word[i] in (a, b, c):\n",
        "      continue\n",
        "    print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "    count += 1\n",
        "    if count >= top:\n",
        "      return\n",
        "\n",
        "def normalize(x):\n",
        "  if x.ndim == 2:\n",
        "    s = np.sqrt((x * x).sum(1))\n",
        "    x /= s.reshape((s.shape[0], 1))\n",
        "  elif x.ndim == 1:\n",
        "    s = np.sqrt((x * x).sum())\n",
        "    x /= s\n",
        "  return x\n",
        "\n",
        "analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go', word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child', word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad', word_to_id, id_to_word, word_vecs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.5859375\n",
            " kid: 4.859375\n",
            " mother: 4.74609375\n",
            " cubans: 4.625\n",
            " hacker: 4.59765625\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " were: 4.703125\n",
            " came: 4.62109375\n",
            " was: 4.41796875\n",
            " 're: 4.2578125\n",
            " went: 4.09375\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " a.m: 6.41015625\n",
            " rape: 5.68359375\n",
            " children: 5.6171875\n",
            " daffynition: 4.87109375\n",
            " incest: 4.8515625\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " rather: 5.7890625\n",
            " more: 5.5703125\n",
            " less: 5.34765625\n",
            " greater: 4.34765625\n",
            " faster: 4.18359375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}