{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "CzAa5QORXnpK",
        "outputId": "b5648adb-9aa9-4e0d-dda5-e78d3b0d0087"
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "\n",
        "def numerical_descent(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  if x.ndim == 1:\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  for x_r in range(x.shape[0]):\n",
        "    for y_r in range(x.shape[1]):\n",
        "      tmp_val = x[x_r, y_r]\n",
        "\n",
        "      x[x_r, y_r] = temp_val + h\n",
        "      fxh1 = f(x)\n",
        "\n",
        "      x[x_r, y_r] = temp_val - h\n",
        "      fxh2 = f(x)\n",
        "\n",
        "      grad[x_r, y_r] = (fxh2 - fxh1) / 2*h\n",
        "\n",
        "      x[x_r, y_r] = tmp_val\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "  \n",
        "  def sigmoid(a):\n",
        "    return 1 / (1 + np.exp(-a))\n",
        "\n",
        "  def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c)\n",
        "    return exp_a / np.sum(exp_a)\n",
        "\n",
        "  def cross_entropy_error(y, t):\n",
        "    return -np.sum(t * np.log(y + 1e-7))\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = self.sigmoid(a1)\n",
        "    a2 = np.dot(z1, w2) + b2\n",
        "    y = self.softmax(a2)\n",
        "    return y\n",
        "  \n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.cross_entropy_error(y, t)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-c08f813125c2>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    W = 0.01 * / np.random.rand(5, 4)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}